{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7547966-26c8-4f52-b5ac-fb1245b5a79d",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc13d08-b5d9-40b4-beb6-e529dad5bdf5",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that is used for regression tasks. It is an extension of the Gradient Boosting framework, which was originally developed for classification problems. Gradient Boosting Regression aims to create a strong regression model by combining multiple weak regression models in an ensemble.       \n",
    "\n",
    "The basic idea behind Gradient Boosting Regression is to iteratively train weak regression models and add them to the ensemble, with each subsequent model focusing on the errors made by the previous models. The algorithm minimizes a loss function by iteratively fitting weak models to the negative gradient of the loss function with respect to the ensemble's predictions.                                                                                                 \n",
    "\n",
    "Here are the main steps involved in Gradient Boosting Regression:\n",
    "\n",
    "1) Initialize the ensemble: The process begins by initializing the ensemble with a constant value, typically the mean of the target variable.\n",
    "\n",
    "2) Calculate the negative gradient: The negative gradient of the loss function with respect to the current ensemble's predictions is computed. The loss function used in regression tasks is typically a differentiable function such as mean squared error (MSE) or mean absolute error (MAE).\n",
    "\n",
    "3) Train a weak regression model: A weak regression model, often a decision tree, is trained to predict the negative gradient obtained in the previous step. The weak model is fitted to minimize the loss function using the input features and the negative gradient as the target variable.\n",
    "\n",
    "4) Update the ensemble: The weak model's predictions are added to the ensemble by multiplying them with a learning rate (also known as the shrinkage parameter) and adding them to the current ensemble's predictions. The learning rate controls the contribution of each weak model to the ensemble and helps prevent overfitting.\n",
    "\n",
    "5) Repeat steps 2-4: The process is repeated for a fixed number of iterations or until a predefined stopping criterion is met. In each iteration, the negative gradient is recalculated based on the current ensemble's predictions, and a new weak model is trained to predict the negative gradient.\n",
    "\n",
    "6) Finalize the ensemble: After the specified number of iterations is completed, the ensemble's predictions are obtained by summing the predictions of all weak models.\n",
    "\n",
    "Gradient Boosting Regression has several advantages, including its ability to handle complex nonlinear relationships, handle missing data, and effectively handle outliers. It is widely used in various regression tasks, ranging from predicting house prices to estimating customer lifetime value. Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and CatBoost, which provide efficient and optimized frameworks for boosting-based regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3da2df-878b-4786-a2c1-636aa5108874",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9326d2-d8fd-4906-be41-371b0e8046e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0144333798733995\n",
      "R-squared: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the ensemble with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(lambda x: initial_prediction)\n",
    "        self.weights.append(1.0)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Train a weak regression model on the negative gradient\n",
    "            weak_model = self.train_weak_model(X, residuals)\n",
    "\n",
    "            # Update the ensemble with the weak model's predictions\n",
    "            self.models.append(weak_model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "\n",
    "    def train_weak_model(self, X, y):\n",
    "        # Fit a weak regression model (decision tree) using X and y\n",
    "        weak_model = lambda x: np.mean(y)  # Example: Using mean as weak model\n",
    "        return weak_model\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by combining the weak models' predictions\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            predictions += weight * model(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Generate a synthetic regression dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gbm.fit(X, y)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = gbm.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "r2 = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e2d29-ed2f-4e51-96d4-620bc87aec13",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16487dff-222a-4b7b-8c85-4d1f3e44e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Score:  11.027395281883704\n",
      "Mean Squared Error: 0.007860345658339318\n",
      "R-squared: 0.9999945110903679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.5)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", -grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff68b54-b868-49b8-bc39-096212865742",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c47ad1-8450-4262-b221-eb7fc9262d95",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a simple or basic model that performs only slightly better than random guessing on a given task. It is called a \"weak\" learner because its individual predictive power is limited. However, when combined with other weak learners through the boosting process, they can contribute to the creation of a strong ensemble model.                                                                                   \n",
    "\n",
    "In Gradient Boosting, weak learners are typically decision trees with shallow depths or few nodes. These decision trees, often referred to as decision stumps, are the most common choice for weak learners due to their simplicity and efficiency. Decision stumps are binary trees with a single split and two leaf nodes. Each weak learner focuses on a specific aspect or subset of the data, capturing a small piece of the overall pattern.                                 \n",
    "\n",
    "During the boosting process, each weak learner is trained on the residuals or errors made by the previous models in the ensemble. The subsequent weak models aim to correct the mistakes made by the previous models by capturing the remaining patterns or patterns missed by the earlier models. By combining the predictions of multiple weak learners, the ensemble gradually improves its predictive power and becomes a strong learner.                                                   \n",
    "\n",
    "The key idea behind using weak learners in Gradient Boosting is that, although each individual model may be weak, the ensemble can achieve high performance by effectively combining their predictions and learning from their collective knowledge. This iterative process of training weak learners and boosting their collective performance forms the basis of Gradient Boosting and allows it to handle complex relationships and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff3144-61de-45e2-ad44-8007e885333f",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830eb054-3e5c-44b6-8bd0-8ce40c2b9120",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1) Combining Weak Learners: The main idea of Gradient Boosting is to combine multiple weak learners (simple models) to create a strong learner (complex model) that can make accurate predictions. Each weak learner focuses on capturing a specific aspect or subset of the data, and the ensemble of weak learners learns from each other's mistakes and combines their individual strengths.\n",
    "\n",
    "2) Iterative Refinement: Gradient Boosting builds the ensemble of weak learners in an iterative and additive manner. Each weak learner is trained to minimize the errors made by the previous models in the ensemble. The algorithm sequentially adds weak learners to the ensemble, with each new learner refining and improving upon the predictions of the previous models.\n",
    "\n",
    "3) Gradient-Based Optimization: Gradient Boosting optimizes the ensemble by minimizing a loss function using gradient descent. It calculates the negative gradient (residuals) of the loss function with respect to the ensemble's predictions and trains the next weak learner to predict the negative gradient. By doing so, each weak learner contributes to reducing the overall error of the ensemble.\n",
    "\n",
    "4) Learning from Residuals: The key insight of Gradient Boosting is that each weak learner focuses on learning the residuals or errors made by the previous models. By training subsequent weak learners on the residuals, the algorithm gradually corrects the mistakes made by earlier models and captures the remaining patterns or information in the data.\n",
    "\n",
    "5) Shrinkage and Weighted Combination: Gradient Boosting introduces a learning rate (or shrinkage parameter) to control the contribution of each weak learner to the ensemble. The learning rate scales the predictions of each weak learner, preventing them from dominating the ensemble. Additionally, each weak learner is assigned a weight based on its performance, and their predictions are combined with these weights to form the final ensemble prediction.\n",
    "\n",
    "The intuition behind Gradient Boosting is to create a powerful ensemble model by iteratively improving upon the predictions of weak models. It leverages the gradient-based optimization and learning from residuals to iteratively refine the ensemble and make accurate predictions. By combining multiple weak models and incorporating their collective knowledge, Gradient Boosting can handle complex patterns in the data and achieve high predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79776a31-083f-477f-8444-44014cd6337d",
   "metadata": {},
   "source": [
    "# 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baad094-9753-49e1-b4f9-42aaba9c83ef",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative and additive manner. Here's a step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "1) Initialize the ensemble: The process begins by initializing the ensemble with a simple model, often a constant value. This initial value can be set as the mean of the target variable.\n",
    "\n",
    "2) Calculate the residuals: The ensemble's predictions are subtracted from the actual target values to calculate the residuals or errors. These residuals represent the information that the current ensemble is not able to capture or predict accurately.\n",
    "\n",
    "4) Train a weak learner on the residuals: A weak learner, typically a decision tree with shallow depth or a decision stump, is trained to predict the residuals. The weak learner is fitted using the input features and the residuals as the target variable. The goal of the weak learner is to capture the patterns or information in the data that can help reduce the residuals.\n",
    "\n",
    "5) Update the ensemble: The predictions of the weak learner are combined with the current ensemble's predictions. However, the weak learner's predictions are not directly added to the ensemble. Instead, they are multiplied by a learning rate (also known as the shrinkage parameter) before being added. The learning rate controls the contribution of each weak learner to the ensemble and helps prevent overfitting.\n",
    "\n",
    "6) Repeat steps 2-4: The process is repeated iteratively for a predetermined number of iterations or until a stopping criterion is met. In each iteration, the residuals are recalculated based on the ensemble's current predictions, and a new weak learner is trained to predict the updated residuals. The predictions of the new weak learner are then added to the ensemble, gradually improving its predictive power.\n",
    "\n",
    "6) Finalize the ensemble: After the specified number of iterations is completed, the ensemble's predictions are obtained by summing the predictions of all weak learners. The final ensemble is a combination of the weak learners, each making a contribution based on its learning rate and performance in reducing the residuals.\n",
    "\n",
    "The key idea behind Gradient Boosting is that each weak learner is trained to correct the mistakes made by the previous models in the ensemble. By focusing on the residuals and iteratively refining the ensemble's predictions, the algorithm gradually builds a strong ensemble that can make accurate predictions by combining the individual strengths of the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e14c66-5990-489c-b0bc-722fcdd39e51",
   "metadata": {},
   "source": [
    "# 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31e3e3-e7df-4706-937d-be7f667e48f8",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves several key steps. Here's an overview of the main steps involved:\n",
    "\n",
    "1) Define the Loss Function: Start by defining a loss function that quantifies the error between the predicted values and the true values. Common loss functions for regression problems include mean squared error (MSE) and mean absolute error (MAE). For classification problems, one can use log loss (binary classification) or cross-entropy loss (multiclass classification).\n",
    "\n",
    "2) Initialize the Ensemble: Initialize the ensemble by setting the initial predictions as a constant value, typically the mean of the target variable for regression problems or the log-odds for classification problems.\n",
    "\n",
    "3) Compute the Negative Gradient (Residuals): Calculate the negative gradient (also known as the pseudo-residuals) of the loss function with respect to the current predictions. The negative gradient represents the direction and magnitude of the error that needs to be corrected by the next weak learner.\n",
    "\n",
    "4) Train a Weak Learner: Fit a weak learner (e.g., decision tree, decision stump) on the negative gradients. The weak learner is trained to predict the negative gradients, aiming to capture the remaining patterns or information that can help reduce the errors made by the current ensemble.\n",
    "\n",
    "5) Update the Ensemble: Update the ensemble by adding the predictions of the weak learner, but not directly. Instead, the predictions are multiplied by a learning rate (also called the shrinkage parameter) before being added to the ensemble. The learning rate controls the contribution of each weak learner, preventing it from dominating the ensemble and allowing for better generalization.\n",
    "\n",
    "Update the Predictions: Update the predictions of the ensemble by summing the predictions of all weak learners seen so far. The updated predictions incorporate the collective knowledge of the weak learners and aim to reduce the errors made by the previous models.\n",
    "\n",
    "Repeat Steps 3-6: Repeat steps 3 to 6 iteratively until a predefined stopping criterion is met. This involves calculating new negative gradients, training additional weak learners, updating the ensemble, and refining the predictions.\n",
    "\n",
    "Final Ensemble: After the desired number of iterations or the stopping criterion is reached, the final ensemble is obtained by combining the predictions of all weak learners in the ensemble. The ensemble's predictions represent the combined knowledge and improved performance achieved through the boosting process.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting lies in minimizing the loss function by iteratively training weak learners to predict the negative gradients. By updating the ensemble with the weighted contributions of the weak learners, the algorithm gradually improves the predictions and reduces the errors. The iterative nature of the algorithm allows it to learn from the mistakes made by previous models and refine the ensemble's predictions over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a4ece-e223-40dd-9311-4894c2e0fbce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83332b51-27ba-4db5-8347-a76007e519ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
