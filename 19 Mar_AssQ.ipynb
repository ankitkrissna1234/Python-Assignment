{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbd13f8-6f71-4c47-97b2-1e93d07212df",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104169b5-fa67-4004-8996-8d777e5b0f02",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features within a specific range. It transforms the data so that it falls between a minimum and maximum value, typically 0 and 1, although it can be adjusted to any desired range. This normalization technique is commonly employed to bring different features onto a similar scale, preventing the dominance of features with larger values and ensuring fair comparisons between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e240bf-ccf4-4060-b1a5-6e4f918b4b1a",
   "metadata": {},
   "source": [
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset with a feature representing the age of individuals, ranging from 20 to 60 years. We want to normalize this feature using Min-Max scaling.\n",
    "\n",
    "Original feature values: [20, 25, 30, 35, 40, 45, 50, 55, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a5f48-e21b-43a6-9d6b-623e5c116d57",
   "metadata": {},
   "source": [
    "Substituting the original feature values into the formula, we get:\n",
    "\n",
    "Scaled feature values: [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]\n",
    "\n",
    "After applying Min-Max scaling, the age values are transformed to fall within the range of 0 to 1. This normalization ensures that the age feature is on the same scale as other features, enabling fair comparisons and preventing potential bias introduced by disparate value ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f0dbf-308f-4215-80f1-d5f580915a6d",
   "metadata": {},
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a0195-dc34-45a6-a1dd-f838a8e0c8f5",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or feature scaling by vector magnitude, is a data preprocessing technique used to scale features by dividing each data point by the Euclidean norm (magnitude) of the feature vector. This normalization technique ensures that the feature vectors have a unit magnitude of 1, essentially transforming them into unit vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff0667-8581-4922-8ea8-328d94e9aeb3",
   "metadata": {},
   "source": [
    "The Unit Vector technique differs from Min-Max scaling in that it doesn't aim to rescale the feature values within a specific range like [0, 1]. Instead, it focuses on normalizing the feature vectors to have a unit magnitude, regardless of the original value range. This technique is particularly useful when the direction or orientation of the feature vectors is important, such as in machine learning algorithms that rely on cosine similarity or distance metrics.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset with two features, representing the height and weight of individuals. We want to normalize these features using the Unit Vector technique.\n",
    "\n",
    "Original feature values:\n",
    "\n",
    "Height: [160, 165, 170, 175, 180]\n",
    "Weight: [55, 60, 65, 70, 75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d345875-61c2-4961-98dd-73ed08a0cda6",
   "metadata": {},
   "source": [
    "Substituting the original feature values into the formula, we get:                                                 \n",
    "\n",
    "Scaled feature values:                                                                                             \n",
    "\n",
    "Height: [0.339, 0.352, 0.366, 0.379, 0.392]                                                                         \n",
    "Weight: [0.304, 0.333, 0.362, 0.391, 0.420]                                                                         \n",
    "After applying the Unit Vector technique, the feature vectors for height and weight are transformed into unit vectors, ensuring that their magnitudes are 1. This normalization technique allows us to focus on the direction or orientation of the feature vectors, disregarding their original scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3c953-88c2-4449-a254-57031eb8cff0",
   "metadata": {},
   "source": [
    "3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e373e-03f2-4cf9-b42c-a2ef9f5e2f4c",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation. It accomplishes this by identifying a new set of orthogonal axes, called principal components, which capture the maximum amount of variance in the data. PCA helps to simplify complex datasets by reducing the number of features while retaining as much information as possible.                       \n",
    "\n",
    "The steps involved in PCA are as follows:                                                                           \n",
    "\n",
    "i) Standardize the data: It is important to standardize the data by subtracting the mean and scaling it to unit variance. This step ensures that all features have the same scale and prevents any single feature from dominating the analysis.                                                                                                       \n",
    "\n",
    "ii) Calculate the covariance matrix: The covariance matrix is computed to understand the relationships between different features in the dataset. It quantifies how changes in one feature correspond to changes in another.       \n",
    "\n",
    "iii) Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are derived from the covariance matrix. Eigenvectors represent the directions or principal components, while eigenvalues indicate the variance explained by each principal component. The eigenvectors are sorted based on their corresponding eigenvalues in descending order.                                                                                                   \n",
    "\n",
    "iv) Select the desired number of principal components: The number of principal components to retain depends on the desired level of dimensionality reduction. A common approach is to choose the principal components that explain a significant portion of the total variance, such as 95% or 99%.                                                     \n",
    "\n",
    "v) Project the data onto the selected principal components: The original data is projected onto the selected principal components, resulting in a lower-dimensional representation of the data. Each data point is represented by its coordinates along the principal components.                                                                 \n",
    " \n",
    "Here's an example to illustrate the application of PCA for dimensionality reduction:                               \n",
    "\n",
    "Suppose we have a dataset with five features: height, weight, age, income, and education level. We want to reduce the dimensionality of the dataset using PCA.                                                                       \n",
    "\n",
    "Original dataset:                                                                                                   \n",
    "\n",
    "i) Height                                                                                                           \n",
    "ii) Weight                                                                                                         \n",
    "iii) Age                                                                                                           \n",
    "iv) Income                                                                                                         \n",
    "v) Education Level                                                                                                 \n",
    "Standardize the data: We subtract the mean and scale each feature to have unit variance.                           \n",
    "\n",
    "Calculate the covariance matrix: We compute the covariance matrix to understand the relationships between the features.                                                                                                           \n",
    "\n",
    "Compute the eigenvectors and eigenvalues: We find the eigenvectors and eigenvalues of the covariance matrix. Let's say we obtain three eigenvectors and eigenvalues.                                                                   \n",
    "\n",
    "Select the desired number of principal components: We decide to retain the top two principal components that explain 90% of the total variance.                                                                                 \n",
    "\n",
    "Project the data onto the selected principal components: We project the original data onto the two selected principal components. Each data point is now represented by its coordinates along these components.                 \n",
    "\n",
    "The dimensionality is reduced from five to two, effectively capturing most of the variance in the data. The reduced dataset can be used for further analysis or visualization, as it retains the most important information while simplifying the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc66b4-a820-4153-8a52-0c93fba7ae15",
   "metadata": {},
   "source": [
    "4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80491a97-f086-43be-9b0f-2b1234b85067",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related, and PCA can be used as a feature extraction technique. Feature extraction aims to transform the original features into a new set of representative features that capture the most relevant information in the data. PCA achieves this by identifying the principal components that explain the maximum variance in the dataset. These principal components can then be used as the extracted features.             \n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:                                         \n",
    "\n",
    "Suppose we have a dataset with 1000 images, each represented by a 100x100 grayscale pixel matrix. Each pixel corresponds to a feature, resulting in a high-dimensional feature space of 10,000 dimensions. We want to extract a smaller set of features that captures the most important information in the images.                                 \n",
    "\n",
    "We can use PCA for feature extraction in the following steps:                                                       \n",
    "\n",
    "i) Flatten the images: We flatten each 100x100 grayscale image into a 1-dimensional array of length 10,000, treating each pixel as a separate feature.                                                                                   \n",
    "\n",
    "ii) Standardize the data: We standardize the flattened images by subtracting the mean and scaling to unit variance.     \n",
    "\n",
    "iii) Apply PCA: We apply PCA to the standardized image data. The algorithm identifies the principal components that explain the maximum variance in the dataset.                                                                       \n",
    "\n",
    "iv) Select the desired number of principal components: We choose the number of principal components that capture a significant amount of variance, such as 95% or 99%.                                                                 \n",
    "\n",
    "v) Project the data onto the selected principal components: We project the standardized image data onto the selected principal components, resulting in a reduced-dimensional representation of the images.                             \n",
    "\n",
    "After applying PCA for feature extraction, we obtain a smaller set of features that are linear combinations of the original pixel values. These extracted features, the principal components, capture the most important information in the images while reducing the dimensionality of the feature space.                                               \n",
    "\n",
    "For example, if we choose to retain 100 principal components, the dimensionality of the feature space is reduced from 10,000 to 100. These 100 principal components can be used as the new set of features for subsequent tasks such as image classification, clustering, or visualization. By extracting the most informative features, PCA helps to simplify the data representation and improve computational efficiency while preserving as much relevant information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161de32-c4df-46f5-b3d3-0a25aa1cf0eb",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64635fd-ffd2-4bc3-9d7f-3e57c46b02ec",
   "metadata": {},
   "source": [
    "To preprocess the features of the food delivery service dataset (price, rating, and delivery time) using Min-Max scaling, you would follow these steps:                                                                             \n",
    "\n",
    "i) Understand the data: Examine the range and distribution of each feature to gain insights into their values and identify any potential outliers or data quality issues.                                                             \n",
    "\n",
    "ii) Standardize the data: Min-Max scaling requires standardizing the features. Calculate the minimum and maximum values of each feature to be used in the scaling formula.                                                           \n",
    "\n",
    "iii) Apply Min-Max scaling: Use the Min-Max scaling formula to transform each feature value within the desired range, typically between 0 and 1.                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b472cd0-a850-4cbd-b604-fd7fa029bcfc",
   "metadata": {},
   "source": [
    "iv) Perform Min-Max scaling for each feature: Apply the Min-Max scaling formula to each feature individually. This ensures that each feature is scaled independently and falls within the desired range.                               \n",
    "\n",
    "For example, let's assume the following original feature values:                                                   \n",
    "\n",
    "Price: [10, 20, 30, 15, 25]                                                                                         \n",
    "Rating: [3.5, 4.2, 4.8, 3.9, 4.5]                                                                                   \n",
    "Delivery Time: [20, 30, 40, 25, 35]                                                                                 \n",
    "To apply Min-Max scaling, you need to calculate the minimum and maximum values of each feature.       \n",
    "\n",
    "For the Price feature, the minimum is 10 and the maximum is 30.                                                     \n",
    "For the Rating feature, the minimum is 3.5 and the maximum is 4.8.                                                 \n",
    "For the Delivery Time feature, the minimum is 20 and the maximum is 40.                                             \n",
    "\n",
    "Then, substitute the values into the Min-Max scaling formula and calculate the scaled values for each feature.     \n",
    "\n",
    "v) Utilize the scaled features for the recommendation system: The scaled features can now be used as inputs for your recommendation system. The Min-Max scaling ensures that each feature is on a similar scale, preventing any dominance from features with larger values. This normalization allows fair comparisons and accurate assessments of each feature's importance within the recommendation algorithm.                                                     \n",
    "\n",
    "By using Min-Max scaling, you transform the original features into a standardized range that preserves the relative relationships between the data points. This preprocessing step facilitates accurate comparisons and fair assessments of the features within the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f503443-1803-4808-9f50-27919ab8e058",
   "metadata": {},
   "source": [
    "6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec854be9-d478-4ed6-966a-7f26019aa0b9",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset containing features like company financial data and market trends for building a stock price prediction model, you can employ PCA (Principal Component Analysis) as a dimensionality reduction technique. Here's how you can use PCA in this context:                                                   \n",
    "\n",
    "1) Standardize the data: Start by standardizing the dataset to ensure that all the features have a similar scale. This step is crucial for PCA as it relies on the variance of the features to determine the principal components.   \n",
    "\n",
    "2) Compute the covariance matrix: Calculate the covariance matrix of the standardized dataset. The covariance matrix represents the relationships and dependencies between the different features.\n",
    "\n",
    "3) Perform PCA: Apply PCA to the covariance matrix or directly to the standardized dataset. PCA will identify the principal components, which are new orthogonal features that capture the maximum amount of variance in the data.\n",
    "\n",
    "4) Determine the number of principal components: Assess the variance explained by each principal component by looking at their corresponding eigenvalues. You can decide to retain a certain percentage of the total variance, such as 95% or 99%.\n",
    "\n",
    "5) Select the desired number of principal components: Choose the number of principal components based on the desired level of dimensionality reduction. Selecting a smaller number of principal components will significantly reduce the dimensionality of the dataset while retaining the most important information.\n",
    "\n",
    "6) Project the data onto the selected principal components: Transform the standardized dataset by projecting it onto the selected principal components. Each data point will be represented by its coordinates along the principal components.\n",
    "\n",
    "By following these steps, PCA reduces the dimensionality of the dataset while preserving the most significant information captured by the principal components. The reduced dataset can be used as input for building a stock price prediction model. It simplifies the feature space, removes redundant information, and potentially enhances the model's performance by eliminating noise or less important features that might hinder prediction accuracy.     \n",
    "\n",
    "It is worth noting that when applying PCA, it is essential to consider the interpretability of the transformed features. In the context of stock price prediction, the interpretability and relevance of the selected principal components to the underlying financial and market factors should be taken into account for better model understanding and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed646e-2746-46a4-8215-43c911ac94a0",
   "metadata": {},
   "source": [
    "7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d838c2-8ea6-4661-a198-900c6b1d3a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate minimum and maximum values\n",
    "X_min = np.min(data)\n",
    "X_max = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling formula\n",
    "scaled_data = (data - X_min) / (X_max - X_min)\n",
    "\n",
    "# Rescale the values to the range of -1 to 1\n",
    "scaled_data_final = 2 * scaled_data - 1\n",
    "\n",
    "print(scaled_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf133c1-c051-478e-9504-65d01e3f7869",
   "metadata": {},
   "source": [
    "8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a1da7-b16f-448f-b188-d5dd0155f57c",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain would depend on various factors, including the specific characteristics of the dataset and the desired level of dimensionality reduction. Here's an approach to determine the number of principal components:                                                                                 \n",
    "\n",
    "1) Standardize the data: Start by standardizing the dataset to ensure that all features have a similar scale. This step is important for PCA as it relies on the variance of the features.\n",
    "\n",
    "2) Apply PCA: Compute the principal components using PCA on the standardized dataset. This will yield a set of orthogonal components that capture the maximum amount of variance in the data.\n",
    "\n",
    "3) Evaluate the explained variance ratio: Examine the explained variance ratio associated with each principal component. The explained variance ratio represents the proportion of the total variance in the data that can be attributed to each principal component. You can access the explained variance ratio using the explained_variance_ratio_ attribute of the PCA object in Python.\n",
    "\n",
    "4) Determine the number of principal components to retain: Decide on the number of principal components to retain based on the desired level of dimensionality reduction. Common approaches include selecting a specific percentage of the total variance to be explained or choosing a predefined number of components.\n",
    "\n",
    "The choice of the number of principal components to retain is subjective and depends on your specific requirements and constraints. However, a common guideline is to select the smallest number of components that capture a significant portion of the total variance. For example, you might choose to retain principal components that explain 95% or 99% of the total variance.                                                                           \n",
    "\n",
    "To determine the number of principal components to retain, you can plot the cumulative explained variance ratio against the number of components and visually inspect the curve. This plot helps you understand how much variance is explained as you increase the number of components and choose the elbow point or the point where the marginal gain in explained variance starts to diminish significantly.                                                       \n",
    "\n",
    "Keep in mind that interpretability is also important in feature extraction. Consider the relevance and interpretability of the retained principal components in the context of the dataset. In the case of gender, it might not be suitable to include it in the PCA analysis, as it is a categorical feature rather than a continuous numerical feature.                                                                                                 \n",
    "\n",
    "In summary, the number of principal components to retain in feature extraction using PCA depends on the desired level of dimensionality reduction, the explained variance ratio, and the interpretability of the retained components in the context of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11481c33-e2fe-408d-a613-de6dd329dbe6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
