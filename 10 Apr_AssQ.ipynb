{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e29a0c-c3be-4453-be6c-6a98db1a6a99",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab0374-acb4-42b5-943d-a244d44e0ac2",
   "metadata": {},
   "source": [
    "To determine the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem.                                                                                                         \n",
    "\n",
    "Let's define the following events:                                                                                     \n",
    "A: Employee uses the health insurance plan.                                                                             \n",
    "S: Employee is a smoker.                                                                                               \n",
    "\n",
    "We are given the following probabilities:                                                                               \n",
    "P(A) = 0.70 (70% of the employees use the health insurance plan)                                                       \n",
    "P(S|A) = 0.40 (40% of the employees who use the plan are smokers)                                                       \n",
    "\n",
    "We want to find P(S|A), the probability that an employee is a smoker given that they use the health insurance plan.                                                                                                                             \n",
    "According to Bayes' theorem:                                                                                           \n",
    "P(S|A) = (P(A|S) * P(S)) / P(A)                                                                                         \n",
    "\n",
    "We need to determine P(A|S), the probability that an employee uses the health insurance plan given that they are a smoker. However, we don't have this information.                                                                       \n",
    "\n",
    "Without further information or assumptions, we cannot calculate the probability of an employee being a smoker given that they use the health insurance plan. Additional data or assumptions about the relationship between smoking and health insurance plan usage would be necessary to make a more precise estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61487d2d-1a89-4776-9aea-63e8da6b5d88",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ac65a-e794-4920-8ba4-44540a279c2a",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in their underlying assumptions and the types of data they are designed to handle.                                                                         \n",
    "\n",
    "1) Bernoulli Naive Bayes:\n",
    "\n",
    "- Assumption: It assumes that the features (input variables) are binary (0/1) variables.\n",
    "- Data Type: It is suitable for binary feature data, where each feature represents the presence or absence of a particular attribute.\n",
    "- Example: It is commonly used in text classification tasks, where the presence or absence of certain words in a document is used as features.\n",
    "\n",
    "2) Multinomial Naive Bayes:\n",
    "\n",
    "- Assumption: It assumes that the features are multinomially distributed.\n",
    "- Data Type: It is suitable for discrete feature data, such as word counts or frequency of occurrence of categorical features.\n",
    "- Example: It is commonly used in text classification tasks where the features are represented by word frequencies or document-term matrices.\n",
    "\n",
    "In both cases, Naive Bayes algorithms assume that the features are conditionally independent given the class label. This is called the \"naive\" assumption and is often violated in practice. However, despite this simplifying assumption, Naive Bayes classifiers can still perform well in many real-world scenarios and are known for their simplicity and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868394d7-2ea5-4171-8375-97d4ea6f3c12",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061ebb-30e1-4264-850d-ebc047aacc28",
   "metadata": {},
   "source": [
    "In Bernoulli Naive Bayes, missing values are typically handled by treating them as a separate category or class for the feature in question. This means that instead of considering a missing value as a binary (0/1) value like the other categories, it is treated as a distinct category on its own.                                                           \n",
    "\n",
    "When training the Bernoulli Naive Bayes classifier, the presence of a missing value is considered as an informative feature value. The classifier learns the probability of each feature category (including the missing category) given the class label.                                                                                                       \n",
    "\n",
    "During prediction, if a feature has a missing value, the classifier incorporates the probability of the missing category into its calculations. It considers the likelihood of the missing category being present or absent based on the training data and classifies the instance accordingly.                                                             \n",
    "\n",
    "It's important to note that the specific implementation of handling missing values in Bernoulli Naive Bayes may vary depending on the software or library used. Some implementations might use alternative approaches such as imputation techniques or treating missing values as a separate class only during training but ignoring them during prediction. Therefore, it is recommended to consult the documentation or source code of the specific implementation you are using for more details on how missing values are handled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1c79c-915c-4d6e-8aba-a2d06823062c",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a8476-5b87-4659-9c92-e294ea929906",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of Naive Bayes algorithm that assumes that the continuous features (input variables) follow a Gaussian distribution (i.e., a normal distribution) within each class.                                                                                 \n",
    "\n",
    "To perform multi-class classification using Gaussian Naive Bayes, the algorithm estimates the mean and standard deviation of each feature for each class. These estimates are used to model the Gaussian distribution for each class and calculate the likelihood of a given feature value belonging to each class.                                         \n",
    "\n",
    "During prediction, the algorithm calculates the probability of an instance belonging to each class using Bayes' theorem and selects the class with the highest probability as the predicted class.                                             \n",
    "\n",
    "Gaussian Naive Bayes is particularly useful when dealing with continuous or real-valued features. It assumes that the features are conditionally independent given the class label, but they follow a Gaussian distribution within each class. This assumption simplifies the calculation of probabilities and makes the algorithm computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a270db-e0ab-41c7-a36b-ae55e6a1add6",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e590f7-fe55-409f-b9af-85f1d86a817e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 score: 0.8481249015095276\n",
      "\n",
      "Performance metrics for Multinomial Naive Bayes\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 score: 0.7282909724016348\n",
      "\n",
      "Performance metrics for Gaussian Naive Bayes\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 score: 0.8130660909542995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "features = data.iloc[:, :-1]\n",
    "labels = data.iloc[:, -1]\n",
    "\n",
    "# Step 2: Create instances of each classifier\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Step 3: Perform 10-fold cross-validation and evaluate performance metrics\n",
    "def evaluate_classifier(classifier, name):\n",
    "    accuracy = cross_val_score(classifier, features, labels, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, features, labels, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, features, labels, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, features, labels, cv=10, scoring='f1').mean()\n",
    "\n",
    "    print(\"Performance metrics for\", name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print()\n",
    "\n",
    "# Step 4: Evaluate each classifier\n",
    "evaluate_classifier(bernoulli_nb, \"Bernoulli Naive Bayes\")\n",
    "evaluate_classifier(multinomial_nb, \"Multinomial Naive Bayes\")\n",
    "evaluate_classifier(gaussian_nb, \"Gaussian Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af5d8c-f676-4fa8-bc6e-99d453f1fb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
