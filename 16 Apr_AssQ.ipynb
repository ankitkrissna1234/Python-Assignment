{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9f4ea2-0859-4872-9540-39d2355ac52a",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2818a-34f0-491b-b1c5-5234cd157f57",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble technique that combines multiple weak models to create a strong predictive model. The main idea behind boosting is to train a sequence of models, where each subsequent model focuses on correcting the mistakes made by the previous models. Boosting is often used for both regression and classification tasks.                                                                                                                 \n",
    "\n",
    "The general boosting algorithm works as follows:                                                                       \n",
    "\n",
    "1) Initially, all training instances are given equal weights.\n",
    "2) A weak model (also called a base learner) is trained on the training data.\n",
    "3) The model's performance is evaluated, and the weights of the training instances are updated to focus more on the instances that were incorrectly predicted by the current model.\n",
    "4) Another weak model is trained, giving more importance to the instances that were misclassified in the previous step.\n",
    "5) Steps 3 and 4 are repeated iteratively for a fixed number of iterations or until a certain condition is met.\n",
    "6) The final prediction is made by combining the predictions of all the weak models, often using a weighted majority voting scheme or weighted averaging.\n",
    "\n",
    "The key idea behind boosting is that by iteratively adjusting the weights of the training instances and training models that focus on the instances that are difficult to classify, the ensemble of weak models can learn to make accurate predictions. Each subsequent model is trained to improve the performance of the previous models, effectively boosting the overall predictive power of the ensemble.                                                                           \n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). These algorithms differ in their specific implementations, but they all follow the general boosting framework.                                                                                                             \n",
    "\n",
    "Boosting has been widely used in practice and has shown excellent performance in various machine learning tasks. It is known for its ability to handle complex patterns in data and improve the predictive accuracy compared to using a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c7ba4-24cc-4bf6-8e7d-d597a02c0e8f",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13641887-d7f2-461b-a496-8835e19b0447",
   "metadata": {},
   "source": [
    "Boosting techniques, such as AdaBoost, Gradient Boosting, and XGBoost, offer several advantages and have proven to be powerful in many machine learning applications. However, like any algorithm, they also have some limitations. Let's explore the advantages and limitations of using boosting techniques:                                                   \n",
    "\n",
    "Advantages of Boosting Techniques:\n",
    "\n",
    "1) Improved Predictive Accuracy: Boosting algorithms often yield highly accurate predictions, as they focus on correcting the mistakes made by previous models. By iteratively refining the model's predictions, boosting can effectively handle complex patterns in data and achieve high accuracy.\n",
    "\n",
    "2) Handling Complex Relationships: Boosting can capture non-linear relationships between features and the target variable. It can learn complex decision boundaries, making it suitable for tasks with intricate patterns and interactions among variables.\n",
    "\n",
    "3) Ensemble of Weak Models: Boosting combines multiple weak models to create a strong ensemble. Weak models, such as decision stumps or shallow decision trees, are computationally efficient and can be easily trained. The ensemble of these models tends to generalize well and reduce overfitting.\n",
    "\n",
    "4) Feature Importance: Boosting algorithms provide a measure of feature importance, indicating which features contribute most to the prediction. This information can be useful for feature selection and understanding the underlying factors driving the predictions.\n",
    "\n",
    "5) Versatility: Boosting techniques can be applied to a wide range of machine learning tasks, including regression, classification, and ranking problems.\n",
    "\n",
    "Limitations of Boosting Techniques:\n",
    "\n",
    "1) Sensitivity to Noisy Data and Outliers: Boosting algorithms can be sensitive to noisy or outlier data points. As boosting aims to correct mistakes made by previous models, outliers or mislabeled instances can disproportionately influence subsequent models and lead to overfitting.\n",
    "\n",
    "2) Computationally Intensive: Boosting involves training a sequence of models iteratively, which can be computationally expensive, especially when dealing with large datasets. The training time and memory requirements increase with the number of iterations and the complexity of the weak models.\n",
    "\n",
    "3) Potential for Overfitting: While boosting can reduce overfitting compared to using a single model, it is still possible to overfit the training data if the boosting process continues for too long. Regularization techniques, such as early stopping or limiting the number of iterations, can help mitigate this issue.\n",
    "\n",
    "4) Model Interpretability: Boosting models tend to be more complex than individual weak models, making their interpretation and explanation more challenging. The ensemble nature of boosting makes it less transparent in terms of understanding the underlying decision-making process.\n",
    "\n",
    "5) Hyperparameter Tuning: Boosting algorithms have several hyperparameters that need to be carefully tuned to achieve optimal performance. Finding the right combination of hyperparameters can be time-consuming and requires experimentation.\n",
    "\n",
    "Despite these limitations, boosting techniques have demonstrated excellent performance in various real-world applications and are widely used in machine learning. It's important to consider these advantages and limitations when deciding whether to use boosting for a particular problem and to carefully tune the hyperparameters based on the specific dataset and task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d4b13-1b8a-4b42-9186-24041100815c",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924bbae-4d03-4b5c-8918-64454ba357fc",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that combines multiple weak models to create a strong predictive model. The main idea behind boosting is to train a sequence of models in an iterative manner, where each subsequent model focuses on correcting the mistakes made by the previous models.                                                   \n",
    "\n",
    "Here is a step-by-step explanation of how boosting works:\n",
    "\n",
    "1) Initialize Sample Weights: Initially, each training instance is assigned an equal weight. These weights represent the importance of the instances during the boosting process.\n",
    "\n",
    "2) Train a Weak Model: A weak model (also called a base learner) is trained on the training data using the initial weights. Common weak models used in boosting are decision stumps (shallow decision trees with only one split) or shallow decision trees with limited depth.\n",
    "\n",
    "3) Evaluate Model Performance: The trained weak model's performance is evaluated by comparing its predictions with the actual target values. The evaluation is typically done using a performance metric such as mean squared error (MSE) for regression tasks or accuracy for classification tasks.\n",
    "\n",
    "4) Update Instance Weights: The weights of the training instances are updated based on their performance. Instances that were incorrectly predicted by the current weak model are assigned higher weights to receive more attention in the subsequent iterations. Correctly predicted instances may have their weights decreased or remain unchanged.\n",
    "\n",
    "5) Build the Next Model: Another weak model is trained using the updated instance weights. The goal of this model is to focus on the instances that were difficult to classify in the previous step. The updated weights guide the model to prioritize the misclassified instances.\n",
    "\n",
    "6) Update Weights and Iterate: Steps 3-5 are repeated iteratively for a fixed number of iterations or until a certain stopping criterion is met. Each subsequent model tries to correct the mistakes made by the previous models, refining the predictions and improving the overall model performance.\n",
    "\n",
    "7) Combine Predictions: Once all the weak models have been trained, their predictions are combined to obtain the final prediction. The combination can be done using different approaches such as weighted majority voting (for classification) or weighted averaging (for regression). The weights of each model can be determined based on their individual performance or other factors.\n",
    "\n",
    "The boosting process aims to create a strong model by sequentially training weak models that focus on the instances that are challenging to classify. By iteratively adjusting the instance weights and training models that address the mistakes of the previous models, boosting leverages the collective knowledge of the ensemble to improve predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eec628-d0a0-4a94-ab89-3b69c6226ae2",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c0a16-6b61-4f88-9ce1-d9f4e123690b",
   "metadata": {},
   "source": [
    "There are several different types of boosting algorithms, each with its own characteristics and variations. Here are some of the most commonly used boosting algorithms:\n",
    "\n",
    "1) AdaBoost (Adaptive Boosting): AdaBoost is one of the earliest and most popular boosting algorithms. It assigns weights to each training instance, and in each iteration, it trains a weak model that focuses on the instances that were previously misclassified. AdaBoost uses exponential loss and combines the predictions of all weak models using weighted majority voting.\n",
    "\n",
    "2) Gradient Boosting: Gradient Boosting is a general framework for boosting that can be used with various loss functions. It involves iteratively training weak models to minimize the loss function's gradient. Common implementations of Gradient Boosting include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "3) XGBoost (Extreme Gradient Boosting): XGBoost is an optimized and highly efficient implementation of Gradient Boosting. It includes additional features such as regularization, parallel processing, and handling missing values. XGBoost is known for its scalability and performance, making it a popular choice for many machine learning tasks.\n",
    "\n",
    "4) LightGBM: LightGBM is another efficient implementation of Gradient Boosting that focuses on achieving faster training speed and lower memory usage. It employs techniques like histogram-based gradient boosting, leaf-wise tree growth, and feature bundling to enhance performance on large datasets.\n",
    "\n",
    "5) CatBoost: CatBoost is a boosting algorithm that is designed to handle categorical features effectively. It incorporates specific handling of categorical variables, such as an advanced feature of target encoding, which allows it to naturally handle categorical data without the need for manual encoding.\n",
    "\n",
    "6) Stochastic Gradient Boosting: Stochastic Gradient Boosting algorithms, like the ones mentioned above, introduce randomness to the boosting process. They utilize techniques such as subsampling (sampling a subset of training instances) and subspace sampling (sampling a subset of features) to speed up training and reduce overfitting.\n",
    "\n",
    "7) LPBoost (Linear Programming Boosting): LPBoost is a boosting algorithm that is based on linear programming techniques. It formulates the boosting problem as a linear program and solves it using optimization methods. LPBoost has a different underlying principle compared to other boosting algorithms and is less commonly used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017e729-91d6-4bcc-8845-75560166c040",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b4b09-08ce-4186-9976-0a68626a080b",
   "metadata": {},
   "source": [
    "Boosting algorithms have several parameters that can be tuned to control the behavior and performance of the boosting process. The specific set of parameters can vary depending on the boosting algorithm you are using, but here are some common parameters found in many boosting algorithms:\n",
    "\n",
    "1) Number of Estimators: This parameter determines the maximum number of weak models (estimators) that will be trained during the boosting process. Increasing the number of estimators allows the boosting algorithm to create a more complex ensemble model but also increases the computational cost.\n",
    "\n",
    "2) Learning Rate: The learning rate (also known as shrinkage) controls the contribution of each weak model to the final ensemble. A lower learning rate forces the algorithm to take smaller steps during the boosting process, which can help improve generalization but may require more iterations to achieve good performance.\n",
    "\n",
    "3) Base Estimator: This parameter specifies the type of weak model that will be used as the base learner. Common choices include decision trees (e.g., decision stumps), linear models, or support vector machines. The base estimator determines the type of weak hypothesis that the boosting algorithm will employ.\n",
    "\n",
    "4) Maximum Depth (Tree-based Boosting): If the base estimator is a decision tree, this parameter defines the maximum depth allowed for the trees in the ensemble. Controlling the maximum depth can help prevent overfitting and limit the complexity of the individual weak models.\n",
    "\n",
    "5) Loss Function: The loss function measures the discrepancy between the predicted values and the actual values. Different boosting algorithms may have specific loss functions associated with their underlying optimization objectives. For example, AdaBoost often uses exponential loss, while Gradient Boosting uses functions like squared loss or absolute loss.\n",
    "\n",
    "6) Subsample Ratio (Stochastic Gradient Boosting): In stochastic gradient boosting variants, such as XGBoost or LightGBM, this parameter specifies the fraction of the training instances to be randomly sampled for each iteration. Subsampling can help reduce overfitting and speed up the training process for large datasets.\n",
    "\n",
    "7) Regularization Parameters: Boosting algorithms may have regularization parameters to control model complexity and reduce overfitting. For example, L1 or L2 regularization can be applied to the weak models or to the weights assigned to the training instances.\n",
    "\n",
    "8)Early Stopping: This technique allows for early termination of the boosting process if the performance on a validation set stops improving. Early stopping helps prevent overfitting and saves computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a7bf7-faa9-4036-ac81-94a8e09a7b53",
   "metadata": {},
   "source": [
    "# 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6fb51-a504-4467-8f68-7393dec29a01",
   "metadata": {},
   "source": [
    "Boosting algorithms combine multiple weak learners to create a strong learner through a weighted combination or ensemble of the weak models' predictions. The exact method of combining the weak learners can vary depending on the specific boosting algorithm, but here are two common approaches:\n",
    "\n",
    "1) Weighted Majority Voting: In classification tasks, each weak learner (e.g., decision stump or tree) makes predictions on the input instances. The final prediction is obtained by combining the predictions of all weak learners, with each weak learner's prediction weighted by its performance or accuracy. The weights can be determined based on the weak learners' individual performance or other factors. The majority voting scheme assigns higher weight to the predictions of more accurate weak learners, effectively giving them a larger influence in the final prediction.\n",
    "\n",
    "2) Weighted Averaging: In regression tasks, the weak learners' predictions are combined through weighted averaging. Each weak learner predicts a numeric value for each input instance, and the final prediction is obtained by taking the weighted average of all the weak learners' predictions. Similarly to weighted majority voting, the weights are often determined based on the weak learners' performance or accuracy.\n",
    "\n",
    "The weights assigned to the weak learners are typically determined based on their individual performance during the boosting process. Weak learners that perform well on the training instances, or those that correctly predict instances with higher weights, are given more influence in the final prediction. The boosting algorithm iteratively adjusts the weights and trains subsequent weak learners to focus on correcting the mistakes made by the previous weak learners, thereby improving the overall predictive performance.                                                                   \n",
    "\n",
    "It's important to note that the specific combination mechanism can vary between different boosting algorithms. For example, AdaBoost uses weighted majority voting, where each weak learner has a weight assigned based on its performance. Gradient Boosting algorithms, on the other hand, use additive combination, where each weak learner's predictions are summed together with a learning rate controlling their contribution.                                   \n",
    "\n",
    "By combining the predictions of multiple weak learners, boosting algorithms leverage the collective knowledge and complementary strengths of the individual models, resulting in a strong learner with improved predictive accuracy and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ed528-34ae-407f-b424-01630d731ade",
   "metadata": {},
   "source": [
    "# 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc980a0-c470-4b78-bc23-e96f8ce0dd34",
   "metadata": {},
   "source": [
    "AdaBoost (Adaptive Boosting) is a popular boosting algorithm that combines multiple weak learners to create a strong learner. It was proposed by Yoav Freund and Robert Schapire in 1996. The key idea behind AdaBoost is to iteratively train weak models and assign weights to the training instances, focusing more on the instances that were previously misclassified. Here is an explanation of how the AdaBoost algorithm works:                                          \n",
    "\n",
    "1) Initialize Sample Weights: Initially, each training instance is assigned an equal weight, denoted by w(i), where i represents the instance index.\n",
    "\n",
    "2) Train a Weak Model: In the first iteration, a weak model (often a decision stump, a shallow decision tree with only one split) is trained on the training data using the initial instance weights.\n",
    "\n",
    "3) Evaluate Model Performance: The trained weak model's performance is evaluated by comparing its predictions with the actual target values. The error rate, denoted by ε, is calculated as the weighted sum of misclassified instances, where the weights are the instance weights.\n",
    "\n",
    "4) Calculate Model Weight: The model weight, α, is computed based on the error rate ε. A smaller error rate leads to a higher model weight, indicating that the model performs better. The model weight is used to determine the influence of the weak model's predictions in the final ensemble.\n",
    "\n",
    "5) Update Instance Weights: The instance weights are updated to focus more on the instances that were misclassified by the current weak model. The misclassified instances have their weights increased, while the correctly classified instances have their weights decreased. The goal is to assign higher weights to the instances that are more difficult to classify, so that subsequent weak models can pay more attention to them.\n",
    "\n",
    "6) Normalize Instance Weights: The instance weights are normalized, so that their sum becomes equal to 1. This normalization ensures that the weights remain valid probabilities.\n",
    "\n",
    "7) Build the Next Model: Another weak model is trained using the updated instance weights. This model focuses on the instances that were weighted more in the previous step. The boosting process iteratively continues for a fixed number of iterations or until a predefined stopping criterion is met.\n",
    "\n",
    "8) Combine Predictions: Once all the weak models have been trained, their predictions are combined to obtain the final prediction. The combination is done using weighted majority voting, where each weak model's prediction is weighted by its corresponding model weight. The final prediction is obtained by summing the weighted predictions and determining the class with the highest sum.\n",
    "\n",
    "By iteratively updating the instance weights and training subsequent weak models that focus on the instances that are difficult to classify, AdaBoost effectively creates a strong learner that leverages the collective knowledge of the ensemble. The final ensemble gives more importance to the predictions of the more accurate weak models, while downweighting the influence of weaker models.                                                                           \n",
    "\n",
    "AdaBoost has shown good performance in various classification tasks and is known for its ability to handle complex patterns in the data. It is widely used in practice and has paved the way for many other boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbce4ce-6781-4e30-a02e-016c7e84ef03",
   "metadata": {},
   "source": [
    "# 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85e3f1-1cd8-467a-bf26-eeda9b7732e0",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm uses an exponential loss function as its default choice. The exponential loss function is a common choice for binary classification problems in boosting algorithms.                                               \n",
    "\n",
    "The exponential loss function is defined as:                                                                           \n",
    "\n",
    "L(y, f(x)) = exp(-y * f(x))                                                                                             \n",
    "\n",
    "Where:\n",
    "\n",
    "- L is the loss function\n",
    "- y is the true label of the instance (either +1 or -1 for binary classification)\n",
    "- f(x) is the predicted score or output of the weak learner for the instance x\n",
    "\n",
    "The exponential loss function assigns a larger penalty when the predicted score (f(x)) and the true label (y) have opposite signs. It penalizes misclassifications more severely, leading the boosting algorithm to focus on instances that are difficult to classify correctly.                                                                               \n",
    "\n",
    "The AdaBoost algorithm aims to minimize the weighted sum of the exponential loss function over all training instances. The weights assigned to the instances are adjusted during the boosting process to focus more on the misclassified instances in subsequent iterations. The weak models are trained to minimize the weighted exponential loss, and their weights are determined based on their individual performance in each iteration.                                         \n",
    "\n",
    "While the exponential loss function is the default choice in AdaBoost, it is important to note that other loss functions can be used in boosting algorithms, depending on the specific problem and requirements. Different loss functions may be more appropriate for different tasks or provide better performance in certain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251ed45-8670-4935-95e8-a4d3301c6232",
   "metadata": {},
   "source": [
    "# 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4eb9a6-3385-4cb5-9e7b-a0106e9245b2",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm updates the weights of misclassified samples to give them higher importance in subsequent iterations. The weight update process can be summarized in the following steps:\n",
    "\n",
    "1) Initialize Sample Weights: Initially, each training instance is assigned an equal weight, denoted by w(i), where i represents the instance index.\n",
    "\n",
    "2) Train a Weak Model: In each iteration of AdaBoost, a weak model (e.g., decision stump) is trained on the training data using the current instance weights.\n",
    "\n",
    "3) Evaluate Model Performance: The trained weak model's performance is evaluated by comparing its predictions with the actual target values. The error rate, denoted by ε, is calculated as the weighted sum of misclassified instances, where the weights are the instance weights.\n",
    "\n",
    "4) Calculate Model Weight: The model weight, α, is computed based on the error rate ε. A smaller error rate leads to a higher model weight, indicating that the model performs better. The model weight is used to determine the influence of the weak model's predictions in the final ensemble.\n",
    "\n",
    "5) Update Instance Weights: The instance weights are updated based on their classification accuracy by the current weak model. The misclassified instances have their weights increased, while the correctly classified instances have their weights decreased or remain unchanged. The update rule is as follows:\n",
    "\n",
    "For misclassified instance i:                                                                                           \n",
    "w(i) = w(i) * exp(α)                                                                                                   \n",
    "\n",
    "For correctly classified instance i:                                                                                   \n",
    "w(i) = w(i) * exp(-α)                                                                                                   \n",
    "\n",
    "Where exp is the exponential function and α is the model weight calculated in step 4.                                   \n",
    "\n",
    "The exponentiated weight update factor ensures that the misclassified instances receive higher weights, while the correctly classified instances receive lower weights. This adjustment prioritizes the misclassified instances in subsequent iterations, allowing subsequent weak models to focus on those instances and try to correct their classification.                                                                                                         \n",
    "\n",
    "6) Normalize Instance Weights: After updating the weights, they are normalized so that their sum becomes equal to 1. This normalization step ensures that the weights remain valid probabilities.     \n",
    "\n",
    "7) Build the Next Model: The boosting process continues by training another weak model using the updated instance weights. This model focuses on the instances that were weighted more in the previous step.\n",
    "\n",
    "The process of updating the weights of misclassified samples allows AdaBoost to emphasize the instances that are difficult to classify correctly. By assigning higher weights to the misclassified instances in each iteration, AdaBoost ensures that subsequent weak models pay more attention to those instances, improving their classification performance. This iterative weight update mechanism is a key aspect of the AdaBoost algorithm and contributes to its ability to effectively handle complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b05ac-ebd4-4f4c-a6af-95d2587aca89",
   "metadata": {},
   "source": [
    "# 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fae3f-acfa-4d7e-b996-bb31093532d1",
   "metadata": {},
   "source": [
    "Increasing the number of estimators in the AdaBoost algorithm has several effects on the model's performance and behavior:\n",
    "\n",
    "1) Improved Predictive Performance: As the number of estimators (weak models) increases, the AdaBoost algorithm has the potential to improve its predictive performance. Adding more weak models allows the algorithm to capture more complex patterns and make more refined predictions. The ensemble of weak models can learn from each other's mistakes and combine their individual strengths, leading to a stronger overall model.\n",
    "\n",
    "2) Longer Training Time: Increasing the number of estimators in AdaBoost also increases the training time. Each additional weak model needs to be trained sequentially, and the algorithm iterates over the entire training set for each estimator. Therefore, training a larger number of estimators can significantly increase the computational cost and training time.\n",
    "\n",
    "3) Higher Risk of Overfitting: Although adding more estimators can improve predictive performance, there is a risk of overfitting the training data if the number of estimators becomes too large. Overfitting occurs when the model becomes too complex and starts to memorize the training instances instead of learning generalizable patterns. Therefore, it is important to monitor the model's performance on a separate validation set and consider early stopping or regularization techniques to prevent overfitting.\n",
    "\n",
    "4) Slower Convergence: Increasing the number of estimators can slow down the convergence of the AdaBoost algorithm. Initially, AdaBoost focuses on the difficult-to-classify instances and assigns higher weights to them. As the boosting process progresses, the algorithm gradually adjusts the instance weights and reduces the influence of the misclassified instances. With a larger number of estimators, it may take more iterations for the boosting process to converge and reach a stable solution.\n",
    "\n",
    "5) Potential for Increased Robustness: In some cases, increasing the number of estimators in AdaBoost can enhance the model's robustness to noise or outliers in the data. By aggregating the predictions of multiple weak models, AdaBoost can reduce the impact of individual errors or noisy instances, leading to more reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a4996-9959-4f59-94ad-2db3ac6d21f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
