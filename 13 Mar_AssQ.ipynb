{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4fcd08-ce7b-4b12-af24-8736273ca165",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ecaec-e3d4-4ad9-8c7a-1e8491b148b0",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of two or more groups to determine if there are statistically significant differences among them. To use ANOVA effectively and ensure the validity of the results, certain assumptions need to be met. Here are the assumptions required for ANOVA:                       \n",
    "\n",
    "i) Independence: The observations within each group or treatment level are assumed to be independent of each other. In other words, the measurements or data points in one group should not be influenced by or dependent on the measurements in another group.                                                                                     \n",
    "\n",
    "ii) Normality: The data within each group should follow a normal distribution. This assumption is important because ANOVA relies on the normality assumption to accurately estimate the population parameters.                         \n",
    "\n",
    "iii) Homogeneity of variances (homoscedasticity): The variability, or spread, of scores within each group should be approximately equal. This means that the variance of the dependent variable should be the same across all groups being compared.                                                                                                                                                                                                                         \n",
    "iv) Homogeneity of regression slopes (for factorial ANOVA): If there are multiple independent variables (factors) in the ANOVA, the interaction between the factors should be similar across all groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f06ea-2cf0-49bc-ae0c-8643046cdb9d",
   "metadata": {},
   "source": [
    "Violations of these assumptions can impact the validity of the ANOVA results. Here are some examples of violations that could affect the validity:                                                                                     \n",
    "\n",
    "i) Violation of independence: If the observations within groups are not independent, such as in a repeated measures design where the same subjects are measured multiple times, the assumption of independence is violated. This can lead to inflated or deflated significance levels, affecting the interpretation of the results.                     \n",
    "\n",
    "ii) Violation of normality: If the data within groups do not follow a normal distribution, ANOVA results may be misleading. Non-normality can affect the accuracy of the p-values and confidence intervals, leading to incorrect conclusions.                                                                                                       \n",
    "\n",
    "iii) Violation of homoscedasticity: When the assumption of equal variances across groups is violated, the standard errors and p-values may be distorted. If one group has significantly higher variability than the others, it can dominate the results, making it difficult to detect true group differences.                                         \n",
    "\n",
    "iv) Violation of homogeneity of regression slopes: In factorial ANOVA, if the interaction between factors is not consistent across groups, it violates the assumption of homogeneity of regression slopes. This can complicate the interpretation of main effects and interaction effects and make it challenging to draw valid conclusions about the factors' effects on the dependent variable.                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5388ce-9c30-4406-bf73-643018914e47",
   "metadata": {},
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6b658-b9c6-4ed2-8d2b-7e55878b0e65",
   "metadata": {},
   "source": [
    "The three types of ANOVA are:                                                                                       \n",
    "\n",
    "i) One-Way ANOVA: This type of ANOVA is used when you have one independent variable (also known as a factor) and one dependent variable. It is used to determine if there are any statistically significant differences in the means of three or more groups. For example, you might use a One-Way ANOVA to compare the effectiveness of three different medications in treating a particular condition.                                                                     \n",
    "\n",
    "ii) Two-Way ANOVA: Two-Way ANOVA is used when you have two independent variables (factors) and one dependent variable. It examines the interaction effects between the two independent variables and their individual effects on the dependent variable. This type of ANOVA is suitable when you want to study how two factors simultaneously influence the outcome. For example, you might use a Two-Way ANOVA to investigate the effects of both gender and age on exam scores.                                                                                                     \n",
    "\n",
    "iii) N-Way ANOVA: N-Way ANOVA is an extension of Two-Way ANOVA and is used when you have more than two independent variables. It allows you to analyze the effects of multiple factors on a dependent variable. N-Way ANOVA is useful when you want to study the combined effects of several factors or when you have complex experimental designs. For example, you might use N-Way ANOVA to analyze the impact of factors like temperature, pH, and time on the growth of different plant species.                                                                                           \n",
    "\n",
    "It's important to select the appropriate type of ANOVA based on the research question, the number of independent variables, and the design of the study. One-Way ANOVA is used when you have a single factor, Two-Way ANOVA is used when you have two factors, and N-Way ANOVA is used when you have more than two factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16daa125-0fc2-4df6-9fb9-aedd4ccb502e",
   "metadata": {},
   "source": [
    "3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af26e2-f01c-4908-a877-91714c81875c",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the division of the total variation observed in the data into different components associated with different sources of variability. These components include the between-group variation and the within-group (or within-treatment) variation. Understanding this concept is crucial because it allows us to quantify and assess the relative contributions of these sources of variation, which helps in drawing meaningful conclusions from ANOVA results.                                                                                     \n",
    "\n",
    "The partitioning of variance is important for the following reasons:                                               \n",
    "\n",
    "i) Identifying sources of variation: By partitioning the total variation, ANOVA helps to identify the sources of variation in the data. It enables researchers to determine whether the observed differences among groups or treatments are due to actual group differences or simply random variability within groups.                         \n",
    "\n",
    "ii) Assessing group differences: ANOVA provides a statistical test to evaluate whether the observed differences between groups are statistically significant. By comparing the between-group variation to the within-group variation, ANOVA quantifies the extent to which the differences among groups are greater than what would be expected by chance.                                                                                                 \n",
    "\n",
    "iii) Estimating effect size: Partitioning the variance allows researchers to estimate the effect size, which provides a measure of the magnitude or practical significance of the group differences. Effect size helps in interpreting the practical importance of the findings and comparing them with other studies.                       \n",
    "\n",
    "iv) Guiding further analyses: The partitioning of variance informs researchers about the sources of variability in the data. This information can guide subsequent analyses, such as post-hoc tests or planned comparisons, to explore specific group differences or patterns.                                                                             \n",
    "\n",
    "v) Study design considerations: Understanding the partitioning of variance can help researchers in planning future studies. By estimating the between-group and within-group variation from previous studies, researchers can determine the required sample size for detecting meaningful group differences in future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad93707-3b40-4e86-a75e-7ed07b4d4895",
   "metadata": {},
   "source": [
    "4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b8f1f3-45d4-446e-afec-f988cfb6bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary libraries:\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22116987-db86-46b8-a5d5-ec96e789947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare the data:\n",
    "# Assuming you have a list or array of observations for each group, combine them into a single array or list.\n",
    "group1 = [1, 2, 3, 4, 5]\n",
    "group2 = [2, 3, 4, 5, 6]\n",
    "group3 = [3, 4, 5, 6, 7]\n",
    "data = np.concatenate([group1, group2, group3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4d637f-d32d-44f6-9f6b-b18ebb27d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the necessary sums of squares:\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "sst = np.sum((data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "sse = np.sum((np.mean(group1) - overall_mean) ** 2) * len(group1)\n",
    "sse += np.sum((np.mean(group2) - overall_mean) ** 2) * len(group2)\n",
    "sse += np.sum((np.mean(group3) - overall_mean) ** 2) * len(group3)\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "ssr = sst - sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5e47e6-14f8-4463-ac7c-5dadf7405be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f80d1f-b04c-4606-b22f-9b6efa0bce04",
   "metadata": {},
   "source": [
    "5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0177b14d-c296-4c8a-8eb7-a93d98e2cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363975a8-9022-4288-90fa-ce79c8771d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor1 = [1, 1, 2, 2, 3, 3]\n",
    "factor2 = [1, 2, 1, 2, 1, 2]\n",
    "dependent_var = [10, 12, 14, 15, 9, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71ebf02-ee77-42ba-a53d-ad2c91d3c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-way ANOVA\n",
    "result = [a * b for a, b in zip(factor1, factor2)]\n",
    "f_value, p_value = stats.f_oneway(dependent_var, factor1, factor2, result)\n",
    "\n",
    "# Calculate the main effects\n",
    "main_effect1 = np.mean(dependent_var) - np.mean(result)\n",
    "main_effect2 = np.mean(dependent_var) - np.mean(result)\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = np.mean(result) - main_effect1 - main_effect2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aacc8da-13a6-4615-9940-5e78b4c2fb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.666666666666668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f72cc8-e4a9-4d2d-b360-be4f51f478d5",
   "metadata": {},
   "source": [
    "6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84c108-838a-44ff-87f5-57d8130391a3",
   "metadata": {},
   "source": [
    "In the given scenario, a one-way ANOVA was conducted, resulting in an F-statistic of 5.23 and a p-value of 0.02.   \n",
    "\n",
    "The interpretation of these results would typically involve comparing the p-value to a predetermined significance level (e.g., Î± = 0.05). If the p-value is less than the significance level, it indicates that the observed differences between the groups are statistically significant.                                                       \n",
    "\n",
    "In this case, since the p-value (0.02) is less than the significance level, we can conclude that there are statistically significant differences between the groups being compared.                                           \n",
    "\n",
    "Furthermore, the F-statistic value (5.23) provides information about the magnitude of the observed differences. The F-statistic represents the ratio of the between-group variability to the within-group variability. A larger F-statistic indicates greater differences between the groups relative to the within-group variability.               \n",
    "\n",
    "To interpret the results, you can say something along the lines of:                                                 \n",
    "\n",
    "\"The one-way ANOVA yielded a statistically significant result (F = 5.23, p = 0.02), indicating that there are significant differences between the groups. The observed differences between the groups are unlikely to have occurred by chance alone. The effect size, as represented by the F-statistic, suggests a moderate to large effect, indicating meaningful distinctions between the groups. Therefore, we can conclude that the groups differ significantly on the variable under investigation.\"                                                                 \n",
    "\n",
    "It's important to note that the interpretation should always consider the context of the study, the research question, and the nature of the data. Additionally, further analyses such as post-hoc tests or planned comparisons may be necessary to identify specific group differences if there are more than two groups involved in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981c0f3-c521-4e29-a936-c93478a162f2",
   "metadata": {},
   "source": [
    "7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15436f5c-f6e0-4351-91a3-811c8ffe3eec",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is an important consideration to ensure accurate and reliable results. Here are some common methods for handling missing data in this context:                                   \n",
    "\n",
    "i)Listwise deletion: In this method, any participant with missing data on any of the variables included in the analysis is completely excluded from the analysis. This approach can result in a loss of valuable data and may introduce bias if the missing data are not missing completely at random (MCAR). It can also reduce the power of the analysis if a large portion of the data is missing.                                                                 \n",
    "\n",
    "ii)Pairwise deletion: With this method, each analysis is based on the available data for that particular comparison. It uses all available data for each specific pair of variables, excluding participants with missing data only for those specific variables. Pairwise deletion allows for the inclusion of more participants and retains more data compared to listwise deletion. However, it can lead to biased estimates if the missing data are not MCAR and can produce different results depending on the specific pairs of variables analyzed.                                   \n",
    "\n",
    "iii)Imputation: Imputation methods involve replacing missing values with estimated values based on the available data. Common imputation techniques include mean imputation (replacing missing values with the mean of the available data), regression imputation (predicting missing values based on the relationship with other variables), and multiple imputation (generating multiple plausible imputed datasets). Imputation methods allow for the retention of all participants and can provide more accurate estimates of the population parameters compared to deletion methods. However, the imputation process introduces some uncertainty and can potentially affect the statistical results if the imputation model is misspecified or if the assumptions of imputation are violated.                             \n",
    "\n",
    "The consequences of using different methods to handle missing data in a repeated measures ANOVA can vary:           \n",
    "\n",
    "Listwise deletion can result in biased estimates if the missing data are not MCAR and reduces the sample size, potentially reducing the power of the analysis.                                                                     \n",
    "Pairwise deletion can produce different results depending on the specific pairs of variables analyzed and may also introduce bias if the missing data are not MCAR.                                                                   \n",
    "Imputation methods can provide more accurate estimates by retaining all participants but introduce uncertainty due to the imputation process. The choice of imputation method and the assumptions made during imputation can impact the results.                                                                                                       \n",
    "It is crucial to carefully consider the missing data pattern, potential reasons for missingness, and the assumptions of the chosen method. Sensitivity analyses and robustness checks are also recommended to assess the robustness of the results under different missing data handling approaches. Consulting with a statistician or using specialized software that handles missing data can help ensure appropriate handling and interpretation of missing data in a repeated measures ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519f8da-1fdd-4024-9139-f7aff25e3e5d",
   "metadata": {},
   "source": [
    "8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df5ab5-747f-4bf7-becb-70bd859ad465",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to make pairwise comparisons between groups to determine which specific groups differ significantly from each other. Here are some common post-hoc tests used after ANOVA and their appropriate usage:                                                 \n",
    "\n",
    "i)Tukey's Honestly Significant Difference (HSD): Tukey's HSD is a conservative post-hoc test that controls for familywise error rate. It compares all possible pairs of group means and provides simultaneous confidence intervals to identify significant differences. It is suitable when you have a balanced design (equal sample sizes) and want to control for Type I error rate across multiple comparisons.                                                       \n",
    "\n",
    "ii)Bonferroni correction: The Bonferroni correction is a simple adjustment method that divides the significance level by the number of comparisons being made. It is commonly used when conducting multiple pairwise comparisons after ANOVA. Each comparison is evaluated against a stricter significance threshold to control the familywise error rate. Bonferroni correction is more conservative compared to other post-hoc tests.                                       \n",
    "\n",
    "iii)Scheffe's test: Scheffe's test is a conservative post-hoc test that can be used when the sample sizes are unequal and the groups have unequal variances. It protects against familywise error rate and offers simultaneous confidence intervals for pairwise comparisons.                                                                                 \n",
    "\n",
    "iv)Dunnett's test: Dunnett's test is used when you have a control group and want to compare each treatment group to the control group. It adjusts the significance level to account for multiple comparisons and controls the familywise error rate.                                                                                             \n",
    "\n",
    "v)Games-Howell test: The Games-Howell test is suitable when the assumptions of equal variances and/or equal sample sizes are violated. It is a robust post-hoc test that allows for unequal variances and sample sizes. It uses a modified t-test procedure to make pairwise comparisons.                                                             \n",
    "\n",
    "Example situation: Suppose you conducted an experiment to compare the effectiveness of four different treatments (A, B, C, and D) on reducing pain levels. After performing an ANOVA, you find a significant overall effect. Now, you want to determine which specific treatments differ significantly from each other. In this case, you would use a post-hoc test, such as Tukey's HSD or Bonferroni correction, to make pairwise comparisons between the treatments and identify which pairs show statistically significant differences in pain reduction.                             \n",
    "\n",
    "Post-hoc tests are valuable in ANOVA to provide more detailed information about the specific group differences, especially when the overall ANOVA result is statistically significant. They help avoid making overly broad conclusions and provide more nuanced insights into the pairwise differences between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333d2e3-b4e4-4d60-b07d-05b4b63838dc",
   "metadata": {},
   "source": [
    "9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de72edb-fddf-4103-8c9c-0d74bf8bd24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 115.37191798598488\n",
      "p-value: 3.5096089739176543e-23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Weight loss data for the three diets\n",
    "diet_A = [2.1, 1.8, 2.5, 1.9, 2.3, 2.6, 1.7, 2.0, 2.4, 2.2, 1.9, 2.1, 2.3, 1.8, 2.0, 2.4, 2.2, 1.7, 2.1, 2.5, 1.9, 2.6, 2.2, 2.3, 2.0]\n",
    "diet_B = [1.4, 1.7, 1.6, 1.5, 1.9, 1.8, 1.6, 1.7, 1.5, 1.9, 1.4, 1.8, 1.7, 1.6, 1.9, 1.8, 1.5, 1.7, 1.6, 1.4, 1.9, 1.5, 1.8, 1.7, 1.6]\n",
    "diet_C = [1.1, 1.3, 1.4, 1.2, 1.0, 1.5, 1.2, 1.1, 1.3, 1.4, 1.2, 1.0, 1.5, 1.3, 1.2, 1.1, 1.4, 1.2, 1.0, 1.5, 1.3, 1.4, 1.2, 1.0, 1.5]\n",
    "\n",
    "# Combine the weight loss data into a single array\n",
    "data = np.concatenate([diet_A, diet_B, diet_C])\n",
    "\n",
    "# Create a corresponding group variable\n",
    "groups = np.repeat(['A', 'B', 'C'], 25)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_value, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_value)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4edac5-98f3-46ba-b9ca-c7ecd8003713",
   "metadata": {},
   "source": [
    "10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d35943b8-11df-42d3-923b-fc4c6d505b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq    df         F    PR(>F)\n",
      "Software               8.506138   2.0  0.805930  0.458397\n",
      "Experience             0.855073   1.0  0.162031  0.690856\n",
      "Software:Experience    8.123378   2.0  0.769665  0.474265\n",
      "Residual             126.653259  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Generate random dataframe\n",
    "n = 30  # Number of employees per group\n",
    "software = np.random.choice(['A', 'B', 'C'], size=n, replace=True)\n",
    "experience = np.random.choice(['Novice', 'Experienced'], size=n, replace=True)\n",
    "time = np.random.normal(loc=12, scale=2, size=n)  # Randomly generated task completion time\n",
    "\n",
    "data = pd.DataFrame({'Software': software, 'Experience': experience, 'Time': time})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Software + Experience + Software:Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the results\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdd58e-20c5-485c-86fd-9255dc1cd33f",
   "metadata": {},
   "source": [
    "11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a4e2ab0-c1d5-4e53-bf9e-ab3d0c71f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -8.509950002570417\n",
      "p-value: 1.397552845346028e-13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Test scores for the control and experimental groups\n",
    "control_scores = [75, 82, 80, 85, 78, 83, 79, 76, 84, 81, 77, 80, 79, 82, 78, 83, 85, 79, 80, 81, 82, 83, 81, 78, 80, 79, 84, 82, 83, 77, 81, 80, 79, 85, 83, 82, 80, 84, 76, 78, 79, 81, 80, 82, 78, 77, 83, 80, 84, 79, 81, 78, 83]\n",
    "experimental_scores = [80, 85, 88, 83, 86, 84, 81, 87, 85, 83, 82, 84, 86, 81, 87, 84, 85, 82, 83, 86, 88, 84, 85, 82, 86, 83, 85, 81, 88, 84, 86, 82, 83, 85, 87, 84, 86, 83, 85, 82, 87, 84, 86, 81, 83, 85, 88, 84, 86, 83, 82, 85, 88]\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8135fa-5dd3-4cea-b4d6-f2b4c41b0a69",
   "metadata": {},
   "source": [
    "12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c4a7c4b-8e9d-49b1-8bb3-5db671d9d571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           sum_sq    df           F        PR(>F)\n",
      "Store     10701.6   2.0  225.062657  4.081295e-35\n",
      "Residual   2068.4  87.0         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sales data for Store A, Store B, and Store C\n",
    "store_a_sales = [100, 110, 120, 130, 115, 105, 112, 118, 122, 108, 113, 119, 117, 121, 109, 112, 111, 119, 114, 108, 107, 120, 110, 113, 111, 116, 119, 105, 114, 112]\n",
    "store_b_sales = [90, 95, 85, 92, 100, 88, 94, 93, 102, 98, 92, 96, 101, 87, 91, 99, 103, 98, 95, 89, 90, 92, 96, 93, 98, 97, 100, 86, 97, 91]\n",
    "store_c_sales = [80, 85, 90, 92, 85, 88, 92, 89, 93, 87, 90, 86, 92, 88, 85, 91, 90, 88, 91, 83, 90, 85, 87, 88, 91, 89, 87, 86, 90, 84]\n",
    "\n",
    "# Combine the sales data into a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Store': ['A'] * len(store_a_sales) + ['B'] * len(store_b_sales) + ['C'] * len(store_c_sales),\n",
    "    'Sales': store_a_sales + store_b_sales + store_c_sales\n",
    "})\n",
    "\n",
    "# Convert the Store column to categorical\n",
    "data['Store'] = pd.Categorical(data['Store'])\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = ols('Sales ~ Store', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the results\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38078e9f-d446-4e27-a500-3a7c1dad84ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
