{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996d4980-48d3-4abe-b167-8cbf6005aec6",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ca4bc-b48c-4173-9b68-e0222672c6a6",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are closely related in machine learning algorithms, particularly in the context of Support Vector Machines (SVMs).                                                                             \n",
    "\n",
    "A polynomial function is a mathematical function that involves raising an input variable to various powers and multiplying them by different coefficients. It has the form:                                                           \n",
    "\n",
    "f(x) = c0 + c1 * x^1 + c2 * x^2 + ... + cn * x^n                                                                       \n",
    "\n",
    "In the context of SVMs, polynomial kernel functions are used to implicitly map the input data into a higher-dimensional feature space, where linear separation may become possible. The polynomial kernel function calculates the dot product between two transformed feature vectors in this higher-dimensional space without explicitly computing the transformed features.                                                                                                               \n",
    " \n",
    "The polynomial kernel function is defined as:                                                                           \n",
    "\n",
    "K(x, x') = (gamma * (x^T * x') + coef0)^degree                                                                         \n",
    "\n",
    "where:                                                                                                                 \n",
    "\n",
    "x and x' are input feature vectors                                                                                     \n",
    "gamma, coef0, and degree are hyperparameters of the kernel function                                                     \n",
    "The polynomial kernel function allows SVMs to capture nonlinear patterns in the original input space by effectively computing the dot product between transformed feature vectors in a higher-dimensional space.                           \n",
    "\n",
    "By choosing an appropriate degree for the polynomial kernel, we can control the flexibility of the decision boundary. Higher degrees allow for more complex decision boundaries, while lower degrees result in simpler decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc070a-7b1c-4adc-abed-54c02f1ba8f4",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389a43a-151c-467c-b607-1b2f1adba820",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using scikit-learn, you can utilize the 'SVC' class and specify the 'kernel' parameter as \"poly\". Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f0bbd3-b057-44c6-b7ca-302c90befa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)  # Set the degree of the polynomial kernel\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee86db-6e03-44d2-94c6-9427496a5b41",
   "metadata": {},
   "source": [
    "In this example, we load the Iris dataset from scikit-learn and split it into training and testing sets using 'train_test_split'.\n",
    "\n",
    "Next, we create an SVM classifier ('svm') using the 'SVC' class. By setting the 'kernel' parameter to \"poly\", we specify that we want to use a polynomial kernel. The degree of the polynomial kernel is set to 3 in this case, but you can adjust it to your desired value.\n",
    "\n",
    "We then train the SVM classifier on the training set using the 'fit' method. After that, we predict the labels for the testing set using the 'predict' method.\n",
    "\n",
    "Finally, we compute the accuracy of the model by comparing the predicted labels with the actual labels using the 'accuracy_score' function.\n",
    "\n",
    "You can experiment with different values for the polynomial degree and other hyperparameters to optimize the SVM's performance for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa4b89-2355-4b22-bfed-fe75f99f4ff2",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1113de7-4ca0-4e53-ac0e-c4c5689ed3ac",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the epsilon parameter, often denoted as ε, determines the width of the epsilon-insensitive tube around the regression line. This tube defines the region where errors within the epsilon boundary are not penalized.                                                                                                         \n",
    "\n",
    "The number of support vectors in SVR is influenced by the value of epsilon. Support vectors are the data points that lie either on the boundary of the epsilon tube or inside it. They contribute to the construction of the regression line and help determine the support vector machine's model.                                                                 \n",
    "\n",
    "When the value of epsilon is increased in SVR, it allows for a wider epsilon-insensitive tube. As a result, more data points can fall within this wider tube without being penalized for their errors. This wider tube provides a larger margin of tolerance for errors, allowing more data points to be considered support vectors.                             \n",
    "\n",
    "Conversely, when the value of epsilon is decreased, the epsilon-insensitive tube becomes narrower, reducing the tolerance for errors. In this case, only data points that are very close to the regression line and fall within the narrower tube will be considered support vectors.                                                                       \n",
    "\n",
    "In summary, increasing the value of epsilon in SVR can lead to an increase in the number of support vectors, as it allows for a wider margin of tolerance for errors and more data points to be considered within the tube. Decreasing epsilon can result in a smaller number of support vectors, as the tolerance for errors becomes narrower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87c42c-36d7-4ba6-9468-5b7fc8b9ccd5",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2e6ee-67f7-4baf-895b-bf9614b69290",
   "metadata": {},
   "source": [
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can significantly impact the performance of the model. Let's explore how each parameter works and discuss scenarios where increasing or decreasing its value may be beneficial:\n",
    "\n",
    "1) Kernel Function:\n",
    "The kernel function determines the type of non-linear mapping applied to the input features. Commonly used kernel functions in SVR include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "- Linear Kernel: Suitable for linearly separable data. No additional parameters need to be adjusted.\n",
    "- Polynomial Kernel: Controlled by the degree parameter. Increasing the degree can capture more complex relationships, but a very high degree may lead to overfitting.\n",
    "- RBF Kernel: Controlled by the gamma parameter. Increasing gamma makes the kernel more peaked, resulting in a narrower decision region and potentially overfitting. Decreasing gamma leads to a smoother decision region and may underfit.\n",
    "- Sigmoid Kernel: Controlled by the gamma and coef0 parameters. Adjusting these parameters can affect the non-linear mapping and decision region. Careful tuning is required to avoid overfitting.\n",
    "\n",
    "2) C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between the model's simplicity and the degree to which errors are tolerated. A larger C value penalizes errors more severely, resulting in a smaller margin and potentially overfitting. A smaller C value allows for a larger margin and more tolerance for errors, which can lead to underfitting.\n",
    "\n",
    "- Increase C: Suitable when the training data is expected to have minimal noise, and you want to prioritize fitting the data points accurately.\n",
    "- Decrease C: Suitable when the training data is noisy or contains outliers, and you want to allow for a larger margin and more tolerance for errors.\n",
    "\n",
    "3) Epsilon Parameter:\n",
    "\n",
    "The epsilon parameter (often denoted as ε) determines the width of the epsilon-insensitive tube. It defines the region where errors within the epsilon boundary are not penalized.\n",
    "\n",
    "- Increase Epsilon: Suitable when you want to allow for larger errors or have no strict requirements on the model's accuracy.\n",
    "- Decrease Epsilon: Suitable when you want the model to fit the data points more closely or have stricter requirements on accuracy.\n",
    "\n",
    "4) Gamma Parameter:\n",
    "\n",
    "The gamma parameter determines the influence of each training example in the SVR model. It controls the shape of the decision region. A higher gamma value makes the model more sensitive to individual data points, potentially overfitting the training data. A lower gamma value makes the model more generalizable, but it may lead to underfitting.\n",
    "\n",
    "- Increase Gamma: Suitable when the training data is expected to have complex relationships or when there are more relevant features. This can result in a tighter decision region and potentially overfitting.\n",
    "- Decrease Gamma: Suitable when the training data is noisy or contains irrelevant features. This can result in a smoother decision region and may reduce overfitting.\n",
    "\n",
    "It's important to note that the impact of these parameters can vary depending on the specific dataset and problem at hand. It is recommended to perform parameter tuning, such as using cross-validation, to find the optimal combination for your specific task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef41a2-df83-47ee-84f5-6b0aa8d0eca8",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3ddd13-adba-4b3f-9e40-1568086ae2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trained_classifier.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data by scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled = scaler.transform(X)  # Scale the entire dataset\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = grid_search.best_estimator_\n",
    "tuned_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'trained_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382d2d5-569e-4b6f-a36d-8561322c1cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
