{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71991d1a-88ed-48a0-89d8-5869d8916876",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbfebc-c2b7-4068-82c5-fdd3a82371aa",
   "metadata": {},
   "source": [
    "In linear regression models, R-squared (or the coefficient of determination) is a statistical measure that assesses the goodness of fit of the model to the observed data. It indicates the proportion of the variance in the dependent variable that can be explained by the independent variables included in the model.                                     \n",
    "\n",
    "R-squared is calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS). The formula for calculating R-squared is:                                                                                   \n",
    "\n",
    "R-squared = ESS / TSS                                                                                                   \n",
    "\n",
    "The explained sum of squares (ESS) represents the variation in the dependent variable that is explained by the independent variables in the model. It is calculated by summing the squared differences between the predicted values and the mean of the dependent variable.                                                                                 \n",
    "\n",
    "The total sum of squares (TSS) represents the total variation in the dependent variable. It is calculated by summing the squared differences between the observed values and the mean of the dependent variable.                             \n",
    "\n",
    "R-squared ranges from 0 to 1. A value of 0 indicates that the model does not explain any of the variation in the dependent variable, while a value of 1 indicates that the model explains all of the variation. Generally, higher values of R-squared indicate a better fit of the model to the data.                                                           \n",
    "\n",
    "However, it's important to note that R-squared alone does not provide a complete picture of the model's performance. It doesn't inform about the reliability of the coefficients or the presence of other influential variables. Therefore, it is often advisable to consider other metrics and diagnostic tests in conjunction with R-squared when evaluating the performance of a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7e439-9367-4588-b133-a37d48b25d06",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d810f-f088-4a29-96c0-7a83c9812ba1",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors (independent variables) in a linear regression model. While R-squared considers only the goodness of fit, adjusted R-squared penalizes the addition of unnecessary predictors.                                                                       \n",
    "\n",
    "The formula for adjusted R-squared is:                                                                                 \n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]                                                     \n",
    "\n",
    "Here, n represents the number of observations in the data set, and p represents the number of predictors in the model. \n",
    "\n",
    "Adjusted R-squared introduces a penalty for including more predictors. It adjusts the R-squared value by subtracting a fraction of the improvement that comes from adding predictors that might not truly improve the model's explanatory power. The penalty increases as the number of predictors increases or the sample size decreases.                       \n",
    "\n",
    "The key difference between adjusted R-squared and regular R-squared is that adjusted R-squared accounts for model complexity and helps prevent overfitting. Overfitting occurs when a model performs well on the data used for training but fails to generalize well to new data. Adjusted R-squared gives more reliable information about the model's performance by considering the balance between the number of predictors and the amount of explained variation. A higher adjusted R-squared indicates a better balance between model complexity and goodness of fit.                             \n",
    "\n",
    "When comparing different models with varying numbers of predictors, adjusted R-squared is often considered a more reliable measure because it provides a more accurate assessment of how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51916579-110d-4dea-ba2c-501da6142dad",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183a157-b2e1-4ab5-a150-e7a0cba1e734",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing and evaluating models with different numbers of predictors or when considering model complexity. Here are a few scenarios where adjusted R-squared is particularly useful:\n",
    "\n",
    "1) Model comparison: When comparing multiple models with different numbers of predictors, regular R-squared may falsely favor models with more predictors, as they tend to have higher R-squared values. Adjusted R-squared, on the other hand, accounts for the number of predictors and provides a fairer comparison by penalizing the inclusion of unnecessary variables. It helps in selecting the model that strikes a balance between explanatory power and complexity.\n",
    "\n",
    "2) Variable selection: Adjusted R-squared can assist in the process of variable selection by guiding the inclusion or exclusion of predictors. As more predictors are added to a model, the regular R-squared is likely to increase, even if the additional variables do not contribute significantly to the model's explanatory power. Adjusted R-squared helps in identifying the optimal subset of predictors that improve the model without introducing excessive complexity.\n",
    "\n",
    "3) Sample size limitations: In situations where the sample size is relatively small compared to the number of predictors, adjusted R-squared becomes more valuable. With a small sample, regular R-squared may overestimate the model's performance due to chance associations between predictors and the dependent variable. Adjusted R-squared adjusts for the potential overfitting by penalizing the inclusion of additional predictors, thereby providing a more conservative estimate of the model's fit.\n",
    "\n",
    "Overall, adjusted R-squared is a valuable tool when comparing models, selecting predictors, and addressing concerns about overfitting or model complexity. It offers a more reliable measure of a model's performance and helps make informed decisions about the inclusion or exclusion of variables in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6c2f3-51dc-4276-bc3d-6d110e74035d",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0215a89-7ec5-448e-8da2-bf12a5f2bf37",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the performance of regression models. They quantify the accuracy of predictions made by the model by measuring the differences between predicted and actual values.                                                                                                                 \n",
    "\n",
    "1) Root Mean Square Error (RMSE):                                                                                         \n",
    "RMSE is a widely used metric that measures the average magnitude of the residuals (the differences between predicted and actual values) in a regression model. It provides a measure of how spread out the residuals are and is particularly sensitive to large errors.                                                                                             \n",
    "RMSE is calculated by taking the square root of the Mean Squared Error (MSE):                                           \n",
    "\n",
    "RMSE = sqrt(MSE)                                                                                                       \n",
    "\n",
    "2) Mean Squared Error (MSE):                                                                                               \n",
    "MSE is another popular metric that represents the average squared difference between predicted and actual values. It amplifies larger errors more than smaller errors due to the squaring operation, making it sensitive to outliers.\n",
    "MSE is calculated by taking the average of the squared residuals:                                                       \n",
    "\n",
    "MSE = (1/n) * Σ(y_actual - y_predicted)^2                                                                               \n",
    "\n",
    "where n is the number of observations, y_actual is the actual value of the dependent variable, and y_predicted is the predicted value.                                                                                                       \n",
    "\n",
    "3) Mean Absolute Error (MAE):\n",
    "MAE is a metric that represents the average absolute difference between predicted and actual values. It measures the average magnitude of errors without considering their direction. MAE is less sensitive to outliers compared to MSE.\n",
    "MAE is calculated by taking the average of the absolute residuals:                                                     \n",
    "\n",
    "MAE = (1/n) * Σ|y_actual - y_predicted|                                                                                 \n",
    "\n",
    "where n is the number of observations, y_actual is the actual value of the dependent variable, and y_predicted is the predicted value.                                                                                                       \n",
    "\n",
    "Interpretation:                                                                                                                                                                                                                                 \n",
    "RMSE, MSE, and MAE all provide a measure of prediction error in regression models, with lower values indicating better model performance. They are all expressed in the same units as the dependent variable, making them interpretable in the context of the problem being addressed. RMSE and MSE give more weight to larger errors, while MAE treats all errors equally.                                                                                                               \n",
    "\n",
    "It's important to note that the choice of which metric to use depends on the specific context and requirements of the problem. RMSE and MSE are often used when larger errors should be penalized more, such as in scenarios where the cost or impact of errors is non-linear. MAE is useful when all errors should be treated equally or when the presence of outliers needs to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fdd71-f4ef-4bb3-a41d-74dbb493cfbe",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae65d8-1670-49ee-938b-8ed4b21d0159",
   "metadata": {},
   "source": [
    "Advantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "1) Interpretability: RMSE, MSE, and MAE are all expressed in the same units as the dependent variable, making them easily interpretable. This allows for straightforward understanding and communication of the magnitude of prediction errors.\n",
    "\n",
    "2) Sensitivity to errors: RMSE and MSE give more weight to larger errors due to the squaring operation, making them sensitive to outliers or large errors. This can be advantageous when it's important to penalize and identify significant deviations between predicted and actual values.\n",
    "\n",
    "3) Balance of errors: MAE treats all errors equally by taking the absolute difference between predicted and actual values. This can be beneficial in situations where all errors are considered equally important, and outliers or extreme values should not disproportionately influence the evaluation.\n",
    "\n",
    "Disadvantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "1) Sensitivity to outliers: While the sensitivity to outliers can be an advantage, it can also be a disadvantage. RMSE and MSE, in particular, amplify the impact of outliers due to the squaring operation. This can lead to inflated error values and potentially misleading model evaluations if outliers are present.\n",
    "\n",
    "2) Lack of robustness: RMSE, MSE, and MAE are all influenced by extreme values and can be sensitive to the distribution of errors. If the error distribution is heavily skewed or contains influential outliers, these metrics may not accurately reflect the model's performance.\n",
    "\n",
    "3) Different scales of measurement: RMSE, MSE, and MAE are all scale-dependent metrics, meaning they can be influenced by the scale of the dependent variable. Comparing models with different scales of measurement or different units may not be straightforward using these metrics alone.\n",
    "\n",
    "4) Lack of information about directionality: RMSE, MSE, and MAE only provide information about the magnitude of errors, not the direction. They do not indicate whether the model tends to overestimate or underestimate the dependent variable. Additional analysis or metrics may be needed to understand the directionality of errors.\n",
    "\n",
    "In practice, it's often recommended to use a combination of evaluation metrics to obtain a more comprehensive understanding of model performance. It is also important to consider the specific context, distribution of errors, and goals of the regression analysis when selecting and interpreting these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5dde9a-daab-42ce-985c-92483d65ce99",
   "metadata": {},
   "source": [
    "# 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984808d2-6134-44ef-a364-4d91f06377d2",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression and other regression models to introduce a penalty on the magnitude of the coefficients. It helps to shrink or eliminate the coefficients of less important predictors by encouraging sparsity in the model.                                                         \n",
    "\n",
    "The key idea behind Lasso regularization is to add a term to the cost function of the regression model that is proportional to the absolute values of the coefficients:                                                               \n",
    "\n",
    "Cost function with Lasso regularization = Sum of squared residuals + lambda * Sum of absolute values of coefficients   \n",
    "\n",
    "Here, lambda is the regularization parameter that controls the amount of regularization applied. A higher lambda value increases the amount of shrinkage applied to the coefficients.                                                         \n",
    "\n",
    "Differences between Lasso regularization and Ridge regularization:\n",
    "\n",
    "1) Penalty term: Lasso regularization uses the L1 norm of the coefficients, which is the sum of their absolute values. In contrast, Ridge regularization (L2 regularization) uses the squared L2 norm of the coefficients, which is the sum of their squared values.\n",
    "\n",
    "2) Feature selection: Lasso regularization has the property of performing feature selection by driving the coefficients of less important predictors to zero. This is because the absolute value penalty can directly reduce coefficients to zero, effectively excluding the corresponding predictors from the model. Ridge regularization, on the other hand, can shrink the coefficients towards zero but doesn't typically lead to exact elimination of predictors.\n",
    "\n",
    "3) Solution path: Lasso regularization tends to produce sparse solutions, meaning it results in models with fewer predictors. As lambda increases, Lasso can drive more coefficients to exactly zero, leading to a simpler model. In contrast, Ridge regularization generally keeps all predictors in the model but with smaller coefficients as lambda increases.\n",
    "\n",
    "Appropriate use of Lasso regularization:                                                                               \n",
    "\n",
    "Lasso regularization is particularly appropriate in situations where there is a belief or evidence that only a subset of predictors truly contribute to the model's predictive power. It is useful when feature selection is desired or when interpretability is important. Lasso can help in reducing model complexity, improving model interpretability, and potentially improving prediction accuracy by eliminating irrelevant or redundant predictors.                           \n",
    "\n",
    "It is worth noting that the choice between Lasso regularization and Ridge regularization (or other regularization techniques) depends on the specific characteristics of the data, the underlying problem, and the goals of the analysis. Experimentation and cross-validation can be employed to determine the optimal regularization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6708d-2ea5-439e-a755-b27cc62805ad",
   "metadata": {},
   "source": [
    "# 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5623d-b0c0-4d17-b785-058bb6139443",
   "metadata": {},
   "source": [
    "While regularized linear models have several benefits in regression analysis, they also have limitations and may not always be the best choice depending on the specific context and characteristics of the data. Here are some limitations to consider:\n",
    "\n",
    "1) Assumption of linearity: Regularized linear models assume a linear relationship between the independent variables and the dependent variable. If the true relationship is nonlinear, regularized linear models may not capture the underlying pattern accurately, leading to suboptimal performance.\n",
    "\n",
    "2) Interpretability: Regularized linear models tend to produce coefficients that are shrunk towards zero, which can make interpretation challenging. When interpretability is crucial, other models like decision trees or rule-based models might be more suitable.\n",
    "\n",
    "3) Feature importance: While regularized linear models can perform feature selection, they do not explicitly provide a measure of feature importance. The coefficient magnitudes alone may not accurately reflect the relative importance of predictors, as regularization can shrink or eliminate coefficients that are informative but have smaller magnitudes.\n",
    "\n",
    "4) Hyperparameter selection: Regularized linear models require tuning of hyperparameters, such as the lambda value in Ridge regression or the lambda value and feature selection threshold in Lasso regression. Selecting appropriate hyperparameters can be challenging and may require cross-validation or other techniques, adding complexity to the modeling process.\n",
    "\n",
    "5) Over-reliance on linear assumptions: Regularized linear models assume that the relationships between variables are linear. If the relationships are nonlinear or involve interactions between variables, regularized linear models may not capture them effectively. Nonlinear models, such as polynomial regression or machine learning algorithms like decision trees or neural networks, may be more appropriate in such cases.\n",
    "\n",
    "6) Outliers and influential observations: Regularized linear models can be sensitive to outliers and influential observations, especially in Ridge regression where the L2 norm is used. Outliers or influential points can have a disproportionate impact on the model's coefficients, potentially affecting the overall performance.\n",
    "\n",
    "7) Computational complexity: In large-scale datasets with a high number of predictors, regularized linear models can be computationally demanding, especially in Lasso regression where feature selection involves solving an optimization problem. Other algorithms, such as gradient boosting or random forests, may provide more efficient solutions for such scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf53bed-e05a-4bb5-8342-ee58b2142f23",
   "metadata": {},
   "source": [
    "# 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13418985-bbe3-47fa-a5d6-f804792007a0",
   "metadata": {},
   "source": [
    "Regularized linear models have several limitations that make them not always the best choice for regression analysis. Here are some limitations to consider:                                                                                 \n",
    "\n",
    "1) Linearity assumption: Regularized linear models assume a linear relationship between the independent variables and the dependent variable. However, many real-world problems exhibit nonlinear relationships, and using linear models may lead to poor performance. In such cases, nonlinear models like decision trees, support vector machines, or neural networks may be more appropriate.\n",
    "\n",
    "2) Interpretability: Regularized linear models, especially those with strong regularization, tend to shrink coefficients towards zero or eliminate them entirely. While this can improve generalization and reduce overfitting, it can make the model less interpretable. Interpreting the impact and importance of specific variables becomes challenging when their coefficients are heavily penalized.\n",
    "\n",
    "3) Feature importance: Regularized linear models do not provide a direct measure of feature importance. The coefficient magnitudes alone may not accurately reflect the true importance of predictors, as regularization can shrink or eliminate coefficients that are informative but have smaller magnitudes. If understanding the relative importance of features is crucial, other techniques like tree-based models or permutation importance analysis may be more suitable.\n",
    "\n",
    "4) Selection of regularization parameter: Regularized linear models require tuning of the regularization parameter (e.g., lambda in Ridge or alpha in Lasso). Selecting the optimal value is not always straightforward and typically involves cross-validation or other techniques. The performance of the model can be sensitive to the chosen parameter value, and selecting an improper value may result in underfitting or overfitting.\n",
    "\n",
    "5) High-dimensional datasets: Regularized linear models may struggle with high-dimensional datasets where the number of predictors is large compared to the number of observations. In such cases, the model may face difficulties in identifying important predictors and distinguishing signal from noise. Alternative methods like dimensionality reduction techniques (e.g., principal component analysis) or nonlinear models may be more appropriate.\n",
    "\n",
    "6) Outliers and influential observations: Regularized linear models can be sensitive to outliers and influential observations, particularly in models with L1 regularization (e.g., Lasso regression). Outliers can disproportionately impact the coefficient estimation, leading to biased results. Robust regression techniques or outlier detection/preprocessing may be necessary in these situations.\n",
    "\n",
    "7) Computational complexity: Regularized linear models, especially those with L1 regularization, involve solving optimization problems that can be computationally demanding, particularly for large-scale datasets with numerous predictors. The computational complexity can become a limitation, requiring significant computational resources and time.\n",
    "\n",
    "In summary, while regularized linear models have many benefits, it is important to consider their limitations and the specific characteristics of the data before choosing them for regression analysis. Depending on the problem at hand, other modeling techniques that address these limitations may provide better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9eacb5-f42b-44b0-bb76-25172a66545a",
   "metadata": {},
   "source": [
    "# 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf983f0-ae8e-4776-9fd0-bd5c091ac617",
   "metadata": {},
   "source": [
    "In comparing the performance of Model A and Model B based on the given evaluation metrics, we need to consider the specific context and requirements of the problem.                                                                       \n",
    "\n",
    "RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are both commonly used metrics in regression analysis, but they measure prediction errors differently. RMSE emphasizes larger errors due to the squared term, while MAE treats all errors equally.                                                                                                         \n",
    "\n",
    "In this case, Model A has an RMSE of 10, indicating an average prediction error of 10 units, while Model B has an MAE of 8, indicating an average prediction error of 8 units.                                                               \n",
    "\n",
    "To determine which model is the better performer, we need to consider the specific context and the nature of the problem. If the problem places more importance on larger errors or outliers, RMSE may be a more appropriate metric. However, if the problem treats all errors equally and does not have a specific emphasis on larger errors, MAE may be more suitable.                                                                                                         \n",
    "\n",
    "Without additional information about the problem and its specific requirements, it is difficult to definitively declare one model as the better performer based solely on the provided metrics. The choice of metric depends on the specific context and the preferences of the stakeholders involved.                                                               \n",
    "\n",
    "Limitations of the choice of metric:                                                                                   \n",
    "\n",
    "1) Sensitivity to outliers: RMSE and MAE can be sensitive to outliers, but RMSE is more influenced by large errors due to the squared term. If the dataset contains outliers that significantly impact the error calculation, it may affect the comparison and interpretation of the models.\n",
    "\n",
    "2) Scale of measurement: The choice of metric should also consider the scale of the dependent variable. RMSE and MAE are both expressed in the same units as the dependent variable, which makes them interpretable. However, if the dependent variable has a specific scale or units that affect the interpretation, it should be considered when evaluating the models.\n",
    "\n",
    "3) Different context, different metrics: Depending on the specific problem, there may be other metrics that are more appropriate for evaluating the models. For example, if the problem involves cost or financial implications, metrics like profit or cost-based evaluation measures may provide more relevant insights.\n",
    "\n",
    "In summary, the choice between Model A and Model B as the better performer depends on the specific context and requirements of the problem. It is essential to consider additional factors, such as the presence of outliers and the scale of the dependent variable when interpreting and comparing the performance of the models based on the provided metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9280e-cf49-464d-9b21-c5b8a5d3e6d6",
   "metadata": {},
   "source": [
    "# 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93644cc8-f92a-4ec0-bb24-197b58fcc1f0",
   "metadata": {},
   "source": [
    "Choosing the better performer between Model A (Ridge regularization) with a regularization parameter of 0.1 and Model B (Lasso regularization) with a regularization parameter of 0.5 depends on the specific context and requirements of the problem. However, there are some considerations and trade-offs to keep in mind when comparing the two regularization methods.                                                                                                               \n",
    "\n",
    "Ridge regularization (L2 regularization) and Lasso regularization (L1 regularization) have some fundamental differences:\n",
    "\n",
    "1) Ridge regularization (L2): Ridge regularization adds a penalty term to the cost function proportional to the sum of squared coefficients. It shrinks the coefficient values towards zero but does not eliminate any of them entirely. The regularization parameter (lambda) controls the amount of regularization, with higher values leading to greater shrinkage.\n",
    "\n",
    "2) Lasso regularization (L1): Lasso regularization adds a penalty term proportional to the sum of the absolute values of the coefficients. It has the property of performing feature selection by driving some coefficients exactly to zero, effectively excluding the corresponding predictors from the model. The regularization parameter (lambda) also controls the amount of regularization, with higher values increasing the likelihood of feature elimination.\n",
    "\n",
    "Considering the provided regularization parameters (0.1 for Ridge and 0.5 for Lasso), it appears that Lasso regularization is applying a stronger regularization compared to Ridge regularization. The higher lambda value in Lasso indicates a greater emphasis on shrinking coefficients towards zero and potentially eliminating less important predictors.                                                                                                             \n",
    "\n",
    "To choose the better performer, one needs to consider the specific context and requirements of the problem. Here are a few considerations:\n",
    "\n",
    "1) Sparsity and feature selection: If feature selection is important and there is a belief or evidence that some predictors are irrelevant or redundant, Lasso regularization (Model B) may be more appropriate. The stronger regularization in Lasso is more likely to eliminate less important predictors by driving their coefficients to zero.\n",
    "\n",
    "2) Interpretability: Ridge regularization (Model A) tends to shrink coefficients towards zero but does not lead to exact elimination. This can make the model more interpretable as all predictors are still included. If interpretability is crucial and the goal is to understand the impact of all predictors, Ridge regularization may be preferred.\n",
    "\n",
    "3) Performance trade-offs: Lasso regularization's ability to eliminate predictors comes with the risk of excluding potentially useful predictors. In cases where some predictors have a small but meaningful impact on the outcome, Lasso regularization may result in slightly inferior performance compared to Ridge regularization, which retains all predictors. Cross-validation or other performance evaluation techniques can help assess the trade-offs between sparsity and predictive performance.\n",
    "\n",
    "4) Computational considerations: Lasso regularization, with its feature selection property, involves solving an optimization problem and can be computationally demanding, especially for large-scale datasets with many predictors. Ridge regularization, on the other hand, has a closed-form solution and is computationally more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac11322-8922-40bf-8089-d2a0591fe54f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
